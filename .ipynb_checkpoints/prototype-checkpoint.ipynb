{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TL;DR – Too Long, Doctor\n",
    "\n",
    "TL;DR is a ML model designed to synthesize and cluster scientific papers. Tailored for both students and researchers seeking to optimize their study time, TL;DR provides a tool to quickly grasp the essence of complex scientific material. Additionally, it caters to those who desire a concise summary or a preliminary overview of a paper before delving into a detailed reading."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import library to extract data from XML file\n",
    "import xml.etree.ElementTree as ET\n",
    "import pandas as pd\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import spacy\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# Download necessary NLTK data\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# Load English tokenizer, tagger, parser, NER and word vectors\n",
    "nlp = spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import BertTokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Global Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "    Extracts data from an XML file and returns it as a dictionary.\n",
    "\n",
    "    Args:\n",
    "        file (str): The path to the XML file.\n",
    "\n",
    "    Returns:\n",
    "        dict: A dictionary containing the extracted data.\n",
    "\"\"\"\n",
    "def extract_data(file):\n",
    "    # Create a dictionary to store the data\n",
    "    data = {}\n",
    "    # Parse the XML file\n",
    "    tree = ET.parse(file)\n",
    "    # Get the root of the XML file\n",
    "    root = tree.getroot()\n",
    "\n",
    "    # Initialize abstract data\n",
    "    data['abstract'] = {}\n",
    "\n",
    "    # Initialize body data\n",
    "    data['body'] = []\n",
    "\n",
    "    # Initialize keywords data\n",
    "    data['keywords'] = []\n",
    "\n",
    "    # Extract title and abstract\n",
    "    article_meta = root.find('.//article-meta')\n",
    "    if article_meta is not None:\n",
    "        title_group = article_meta.find('title-group')\n",
    "        data['title'] = title_group.find('article-title').text if title_group is not None else None\n",
    "\n",
    "        abstract_section = article_meta.find('abstract')\n",
    "        if abstract_section is not None:\n",
    "            for section in abstract_section.findall('sec'):\n",
    "                section_title = section.find('title').text if section.find('title') is not None else ''\n",
    "                section_text = section.find('p').text if section.find('p') is not None else ''\n",
    "                if 'simple summary' in section_title.lower():\n",
    "                    data['abstract']['simple_summary'] = section_text\n",
    "                elif 'abstract' in section_title.lower():\n",
    "                    data['abstract']['abstract'] = section_text\n",
    "\n",
    "        # Extract keywords\n",
    "        kwd_group = article_meta.find('kwd-group')\n",
    "        if kwd_group is not None:\n",
    "            data['keywords'] = [kwd.text for kwd in kwd_group.findall('kwd') if kwd.text]\n",
    "\n",
    "    # Extract body sections\n",
    "    body_section = root.find('body')\n",
    "    if body_section is not None:\n",
    "        for sec in body_section.findall('sec'):\n",
    "            section_data = {\n",
    "                'title': sec.find('title').text if sec.find('title') is not None else None,\n",
    "                'content': [p.text for p in sec.findall('p') if p.text]\n",
    "            }\n",
    "            data['body'].append(section_data)\n",
    "\n",
    "    # Return the extracted data\n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract data from the XML files\n",
    "# Create a list to store the data\n",
    "data = []\n",
    "# Get the path of the XML files\n",
    "path = './data'\n",
    "\n",
    "# Get the list of the XML files\n",
    "files = os.listdir(path)\n",
    "\n",
    "# Loop through the XML files\n",
    "for file in files:\n",
    "    # Extract data from the XML file\n",
    "    data.append(extract_data(path + '/' + file))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the list of dictionaries to a Pandas DataFrame\n",
    "df = pd.DataFrame(data)\n",
    "# Save the DataFrame as a JSON file\n",
    "df.to_json('data.json', orient='records')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# iterate over data list to print its contents\n",
    "for i in range(len(data)):\n",
    "    # print the title of the article\n",
    "    print(data[i]['title'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(data)):\n",
    "    # print the abstract of the article\n",
    "    print(data[i]['abstract'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(data)):\n",
    "    # print the body of the article\n",
    "    print(data[i]['body'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Entra dentro body per combinare tutti i paragrafi in un unico testo\n",
    "# Cicla su tutti i dizonari dentro body, che sono i paragrafi\n",
    "# I paragrafi sono un dizionario con chiave title e content\n",
    "# Title contiene una stringa con il titolo del paragrafo\n",
    "# Content è una lista di stringhe che vanno combinate in un unico testo\n",
    "# Cicla su content e combina tutte le stringhe in un unico testo\n",
    "\n",
    "for i in range(len(data)):\n",
    "    # print each element of body inside data\n",
    "    for section in data[i][\"body\"]:\n",
    "        # print the title of the section\n",
    "        print(section[\"title\"])\n",
    "        # print the content of the section\n",
    "        print(section[\"content\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def combine_body_content(body_list):\n",
    "    combined_content = []\n",
    "\n",
    "    # Verifica che il body sia una lista\n",
    "    if isinstance(body_list, list):\n",
    "        # Cicla su tutti i dizionari dentro body, che sono i paragrafi\n",
    "        for section in body_list:\n",
    "            # Ottieni il titolo e il contenuto della sezione, se esistente\n",
    "            title = section.get('title')\n",
    "            content = ' '.join(section.get('content', []))\n",
    "            # Combina il titolo e il contenuto con uno spazio e aggiungi al contenuto combinato\n",
    "            combined_section = ' '.join(filter(None, [title, content])).strip()\n",
    "            combined_content.append(combined_section)\n",
    "    # Unisci tutte le sezioni in una singola stringa separata da spazi\n",
    "    return ' '.join(combined_content)\n",
    "\n",
    "# Applica la funzione alla colonna \"body\" del dataframe\n",
    "df['combined_body'] = df['body'].apply(combine_body_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verifica il contenuto della nuova colonna \"combined_body\"\n",
    "print(df['combined_body'].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(data)):\n",
    "    # print the keywords of the article\n",
    "    print(data[i]['keywords'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(type(df.loc[0, 'abstract']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df.dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Funzione per combinare \"simple summary\" e \"abstract\" gestendo i valori None\n",
    "def combine_abstract(abstract):\n",
    "    if isinstance(abstract, dict):\n",
    "        simple_summary = abstract.get('simple_summary') or ''  # Restituisce una stringa vuota se il valore è None\n",
    "        abstract_text = abstract.get('abstract') or ''  # Restituisce una stringa vuota se il valore è None\n",
    "        return ' '.join([simple_summary, abstract_text]).strip()\n",
    "    return ''\n",
    "\n",
    "# Applica la funzione a ciascuna riga della colonna \"abstract\"\n",
    "df['combined_abstract'] = df['abstract'].apply(combine_abstract)\n",
    "\n",
    "# Verifica il risultato\n",
    "print(df['combined_abstract'].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df.dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stampa i primi elementi della colonna \"combined_abstract\"\n",
    "print(df['combined_abstract'].head())\n",
    "\n",
    "# Verifica il tipo del primo elemento della colonna \"combined_abstract\"\n",
    "print(type(df.loc[0, 'combined_abstract']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Classification\n",
    "\n",
    "For the Text Classification task we will use BERT."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preparing keywords column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "\n",
    "# Sostituisci i valori NaN con una stringa vuota\n",
    "df['keywords'] = df['keywords'].fillna('')\n",
    "\n",
    "# Assicurati che tutti i valori nella colonna 'keywords' siano stringhe\n",
    "df['keywords'] = df['keywords'].astype(str)\n",
    "\n",
    "# Converti le stringhe di parole chiave in liste di parole chiave\n",
    "df['keywords_list'] = df['keywords'].apply(lambda x: x.split(',') if x else [])\n",
    "\n",
    "# Inizializza il MultiLabelBinarizer\n",
    "mlb = MultiLabelBinarizer()\n",
    "\n",
    "# Adatta il MultiLabelBinarizer alle liste di parole chiave e trasformale in vettori binari\n",
    "labels = mlb.fit_transform(df['keywords_list'])\n",
    "\n",
    "# Ora labels è un array binario che rappresenta la presenza/assenza di ciascuna parola chiave\n",
    "# Si possono usare queste etichette per addestrare BERT\n",
    "\n",
    "# Per vedere a quali parole chiave corrispondono le colonne in labels\n",
    "print(mlb.classes_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converti l'array NumPy labels in un tensore PyTorch\n",
    "labels_tensor = torch.tensor(labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preparing body column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer\n",
    "\n",
    "# Carica il tokenizzatore di BERT\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# Funzione per tokenizzare un testo con BERT\n",
    "def tokenize_with_bert(text):\n",
    "    return tokenizer.encode_plus(\n",
    "        text,\n",
    "        add_special_tokens=True,  # Aggiungi '[CLS]' e '[SEP]'\n",
    "        max_length=512,  # Imposta la massima lunghezza dei token\n",
    "        padding='max_length',  # Aggiungi padding per raggiungere la massima lunghezza\n",
    "        truncation=True,  # Tronca i token in eccesso\n",
    "        return_attention_mask=True,  # Restituisci la maschera di attenzione\n",
    "        return_tensors='pt'  # Restituisci tensori PyTorch\n",
    "    )\n",
    "\n",
    "# Applica la tokenizzazione al corpo combinato degli articoli\n",
    "df['bert_input_body'] = df['combined_body'].apply(lambda x: tokenize_with_bert(x))\n",
    "\n",
    "# Estrai i token e le maschere di attenzione per l'addestramento\n",
    "input_ids_body = torch.cat([item['input_ids'] for item in df['bert_input_body']])\n",
    "attention_masks_body = torch.cat([item['attention_mask'] for item in df['bert_input_body']])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Controlliamo il dataframe per vedere che non ci siano anomalie"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(input_ids_body.size())\n",
    "print(attention_masks_body.size())\n",
    "print(labels.size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modelling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ora che abbiamo preparato le etichette con MultiLabelBinarizer e tokenizzato il corpo dell'articolo nella colonna combined_body, il prossimo passo è strutturare questi dati in un formato che BERT possa utilizzare per l'addestramento. \n",
    "\n",
    "Ciò implica la creazione di un dataset PyTorch con i tokenizzati input_ids, le attention_masks e le etichette binarizzate.\n",
    "\n",
    "Seguiremo questi passaggi:\n",
    "\n",
    "1. Creazione del dataset PyTorch (TensorDataset che combina input_ids_body, attention_masks_body, labels)\n",
    "2. Suddivisione training e validation set\n",
    "3. Creazione dei DataLoader\n",
    "4. Caricamento e configurazione di BERT\n",
    "5. Traning di BERT\n",
    "6. Valutazione di BERT\n",
    "7. Salvataggio del modello"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
    "\n",
    "# Converti labels in un tensore PyTorch\n",
    "labels_tensor = torch.tensor(labels)\n",
    "\n",
    "# Crea il TensorDataset\n",
    "dataset = TensorDataset(input_ids_body, attention_masks_body, torch.tensor(labels))\n",
    "\n",
    "# Suddividi il dataset in set di addestramento e validazione (90-10)\n",
    "train_size = int(0.9 * len(dataset))\n",
    "val_size = len(dataset) - train_size\n",
    "train_dataset, val_dataset = torch.utils.data.random_split(dataset, [train_size, val_size])\n",
    "\n",
    "# Crea il DataLoader per il set di addestramento\n",
    "train_dataloader = DataLoader(\n",
    "    train_dataset,\n",
    "    sampler = RandomSampler(train_dataset),\n",
    "    batch_size = 4\n",
    ")\n",
    "\n",
    "# Crea il DataLoader per il set di validazione\n",
    "validation_dataloader = DataLoader(\n",
    "    val_dataset,\n",
    "    sampler = SequentialSampler(val_dataset),\n",
    "    batch_size = 4  \n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ora che abbiamo i DataLoader pronti, carichiamo BERT e lo prepariamo per il training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
