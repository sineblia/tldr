<!DOCTYPE article
PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD with MathML3 v1.3 20210610//EN" "JATS-archivearticle1-3-mathml3.dtd">
<article xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" dtd-version="1.3" xml:lang="en" article-type="research-article"><?properties open_access?><processing-meta base-tagset="archiving" mathml-version="3.0" table-model="xhtml" tagset-family="jats"><restricted-by>pmc</restricted-by></processing-meta><front><journal-meta><journal-id journal-id-type="nlm-ta">Animals (Basel)</journal-id><journal-id journal-id-type="iso-abbrev">Animals (Basel)</journal-id><journal-id journal-id-type="publisher-id">animals</journal-id><journal-title-group><journal-title>Animals : an Open Access Journal from MDPI</journal-title></journal-title-group><issn pub-type="epub">2076-2615</issn><publisher><publisher-name>MDPI</publisher-name></publisher></journal-meta><article-meta><article-id pub-id-type="pmc">PMC10000094</article-id><article-id pub-id-type="doi">10.3390/ani13050838</article-id><article-id pub-id-type="publisher-id">animals-13-00838</article-id><article-categories><subj-group subj-group-type="heading"><subject>Article</subject></subj-group></article-categories><title-group><article-title>A Lightweight Automatic Wildlife Recognition Model Design Method Mitigating Shortcut Learning</article-title></title-group><contrib-group><contrib contrib-type="author"><name><surname>Zhong</surname><given-names>Yujie</given-names></name><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Methodology" vocab-term-identifier="https://credit.niso.org/contributor-roles/methodology/">Methodology</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Software" vocab-term-identifier="https://credit.niso.org/contributor-roles/software/">Software</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Validation" vocab-term-identifier="https://credit.niso.org/contributor-roles/validation/">Validation</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Formal analysis" vocab-term-identifier="https://credit.niso.org/contributor-roles/formal-analysis/">Formal analysis</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Data curation" vocab-term-identifier="https://credit.niso.org/contributor-roles/data-curation/">Data curation</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Writing &#x02013; original draft" vocab-term-identifier="https://credit.niso.org/contributor-roles/writing-original-draft/">Writing &#x02013; original draft</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Writing &#x02013; review &#x00026; editing" vocab-term-identifier="https://credit.niso.org/contributor-roles/writing-review-editing/">Writing &#x02013; review &#x00026; editing</role><xref rid="af1-animals-13-00838" ref-type="aff">1</xref><xref rid="fn1-animals-13-00838" ref-type="author-notes">&#x02020;</xref></contrib><contrib contrib-type="author"><name><surname>Li</surname><given-names>Xiao</given-names></name><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Conceptualization" vocab-term-identifier="https://credit.niso.org/contributor-roles/conceptualization/">Conceptualization</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Methodology" vocab-term-identifier="https://credit.niso.org/contributor-roles/methodology/">Methodology</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Software" vocab-term-identifier="https://credit.niso.org/contributor-roles/software/">Software</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Validation" vocab-term-identifier="https://credit.niso.org/contributor-roles/validation/">Validation</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Formal analysis" vocab-term-identifier="https://credit.niso.org/contributor-roles/formal-analysis/">Formal analysis</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Investigation" vocab-term-identifier="https://credit.niso.org/contributor-roles/investigation/">Investigation</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Writing &#x02013; original draft" vocab-term-identifier="https://credit.niso.org/contributor-roles/writing-original-draft/">Writing &#x02013; original draft</role><xref rid="af1-animals-13-00838" ref-type="aff">1</xref><xref rid="fn1-animals-13-00838" ref-type="author-notes">&#x02020;</xref></contrib><contrib contrib-type="author"><contrib-id contrib-id-type="orcid" authenticated="true">https://orcid.org/0000-0003-1367-324X</contrib-id><name><surname>Xie</surname><given-names>Jiangjian</given-names></name><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Conceptualization" vocab-term-identifier="https://credit.niso.org/contributor-roles/conceptualization/">Conceptualization</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Writing &#x02013; review &#x00026; editing" vocab-term-identifier="https://credit.niso.org/contributor-roles/writing-review-editing/">Writing &#x02013; review &#x00026; editing</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Supervision" vocab-term-identifier="https://credit.niso.org/contributor-roles/supervision/">Supervision</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Project administration" vocab-term-identifier="https://credit.niso.org/contributor-roles/project-administration/">Project administration</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Funding acquisition" vocab-term-identifier="https://credit.niso.org/contributor-roles/funding-acquisition/">Funding acquisition</role><xref rid="af1-animals-13-00838" ref-type="aff">1</xref><xref rid="af2-animals-13-00838" ref-type="aff">2</xref><xref rid="c1-animals-13-00838" ref-type="corresp">*</xref></contrib><contrib contrib-type="author"><contrib-id contrib-id-type="orcid" authenticated="true">https://orcid.org/0000-0002-2267-786X</contrib-id><name><surname>Zhang</surname><given-names>Junguo</given-names></name><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Writing &#x02013; review &#x00026; editing" vocab-term-identifier="https://credit.niso.org/contributor-roles/writing-review-editing/">Writing &#x02013; review &#x00026; editing</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Project administration" vocab-term-identifier="https://credit.niso.org/contributor-roles/project-administration/">Project administration</role><role vocab="credit" vocab-identifier="https://credit.niso.org/" vocab-term="Funding acquisition" vocab-term-identifier="https://credit.niso.org/contributor-roles/funding-acquisition/">Funding acquisition</role><xref rid="af1-animals-13-00838" ref-type="aff">1</xref><xref rid="af2-animals-13-00838" ref-type="aff">2</xref><xref rid="c1-animals-13-00838" ref-type="corresp">*</xref></contrib></contrib-group><contrib-group><contrib contrib-type="editor"><name><surname>Febbraro</surname><given-names>Mirko Di</given-names></name><role>Academic Editor</role></contrib></contrib-group><aff id="af1-animals-13-00838"><label>1</label>School of Technology, Beijing Forestry University, Beijing 100083, China</aff><aff id="af2-animals-13-00838"><label>2</label>Research Center for Biodiversity Intelligent Monitoring, Beijing Forestry University, Beijing 100083, China</aff><author-notes><corresp id="c1-animals-13-00838"><label>*</label>Correspondence: <email>shyneforce@bjfu.edu.cn</email> (J.X.); <email>zhangjunguo@bjfu.edu.cn</email> (J.Z.)</corresp><fn id="fn1-animals-13-00838"><label>&#x02020;</label><p>These authors contributed equally to this work.</p></fn></author-notes><pub-date pub-type="epub"><day>25</day><month>2</month><year>2023</year></pub-date><pub-date pub-type="collection"><month>3</month><year>2023</year></pub-date><volume>13</volume><issue>5</issue><elocation-id>838</elocation-id><history><date date-type="received"><day>13</day><month>12</month><year>2022</year></date><date date-type="rev-recd"><day>04</day><month>2</month><year>2023</year></date><date date-type="accepted"><day>21</day><month>2</month><year>2023</year></date></history><permissions><copyright-statement>&#x000a9; 2023 by the authors.</copyright-statement><copyright-year>2023</copyright-year><license><ali:license_ref xmlns:ali="http://www.niso.org/schemas/ali/1.0/" specific-use="textmining" content-type="ccbylicense">https://creativecommons.org/licenses/by/4.0/</ali:license_ref><license-p>Licensee MDPI, Basel, Switzerland. This article is an open access article distributed under the terms and conditions of the Creative Commons Attribution (CC BY) license (<ext-link ext-link-type="uri" xlink:href="https://creativecommons.org/licenses/by/4.0/">https://creativecommons.org/licenses/by/4.0/</ext-link>).</license-p></license></permissions><abstract><sec><title>Simple Summary</title><p>Due to the complexity of the wild environment, wildlife recognition based on camera trap images is challenging. Indeed, as the backgrounds of images captured from the same infrared camera trap are rather similar, shortcut learning of recognition models are produced, resulting in reduced generality and poor recognition model performance. Therefore, we propose a data augmentation strategy that integrates image synthesis (IS) and regional background suppression (RBS). This strategy alleviates a model&#x02019;s focus on the background, guiding it to focus on the wildlife in order to improve the model&#x02019;s generality, resulting in better recognition performance. Furthermore, in order to offer the lightweight recognition model for deep learning-based real-time wildlife monitoring on edge devices, we developed a model compression strategy that combines adaptive pruning and knowledge distillation. The produced lightweight model can reduce the computational effort of wildlife recognition with less loss of accuracy and is beneficial for real-time wildlife monitoring with the use of edge intelligence.</p></sec><sec><title>Abstract</title><p>Recognizing wildlife based on camera trap images is challenging due to the complexity of the wild environment. Deep learning is an optional approach to solve this problem. However, the backgrounds of images captured from the same infrared camera trap are rather similar, and shortcut learning of recognition models occurs, resulting in reduced generality and poor recognition model performance. Therefore, this paper proposes a data augmentation strategy that integrates image synthesis (IS) and regional background suppression (RBS) to enrich the background scene and suppress the existing background information. This strategy alleviates the model&#x02019;s focus on the background, guiding it to focus on the wildlife in order to improve the model&#x02019;s generality, resulting in better recognition performance. Furthermore, to offer a lightweight recognition model for deep learning-based real-time wildlife monitoring on edge devices, we develop a model compression strategy that combines adaptive pruning and knowledge distillation. Specifically, a student model is built using a genetic algorithm-based pruning technique and adaptive batch normalization (GA-ABN). A mean square error (MSE) loss-based knowledge distillation method is then used to fine-tune the student model so as to generate a lightweight recognition model. The produced lightweight model can reduce the computational effort of wildlife recognition with only a 4.73% loss in accuracy. Extensive experiments have demonstrated the advantages of our method, which is beneficial for real-time wildlife monitoring with edge intelligence.</p></sec></abstract><kwd-group><kwd>automatic wildlife recognition</kwd><kwd>shortcut learning</kwd><kwd>data augmentation</kwd><kwd>model compression</kwd></kwd-group><funding-group><award-group><funding-source>Fundamental Research Funds for the Central Universities</funding-source><award-id>2021ZY70</award-id></award-group><award-group><funding-source>Beijing Municipal Natural Science Foundation</funding-source><award-id>6214040</award-id></award-group><funding-statement>This work was jointly supported by the Fundamental Research Funds for the Central Universities (2021ZY70) and the Beijing Municipal Natural Science Foundation (6214040).</funding-statement></funding-group></article-meta></front><body><sec sec-type="intro" id="sec1-animals-13-00838"><title>1. Introduction</title><p>Accurate wildlife density and abundance monitoring assists in analysis of the causes of biodiversity loss and assessment of the impacts of conservation measures [<xref rid="B1-animals-13-00838" ref-type="bibr">1</xref>]. According to the International Union for Conservation of Nature (IUCN), up to 17,000 species are considered as &#x0201c;data deficient&#x0201d; [<xref rid="B2-animals-13-00838" ref-type="bibr">2</xref>]. Therefore, there is an urgent need for effective large-scale wildlife monitoring systems with great spatiotemporal resolution. Camera traps have become an essential tool for wildlife monitoring in the recent decades, collecting huge amounts of data every day [<xref rid="B3-animals-13-00838" ref-type="bibr">3</xref>]. Because manual annotation of such huge amounts of data is time-consuming, automatic wildlife recognition is an appealing method for analyzing these data [<xref rid="B4-animals-13-00838" ref-type="bibr">4</xref>,<xref rid="B5-animals-13-00838" ref-type="bibr">5</xref>]. Deep learning methods have recently emerged as the dominant method for automatically recognizing wildlife. Xie et al. [<xref rid="B6-animals-13-00838" ref-type="bibr">6</xref>] introduced the SE-ResNeXt model for recognizing 26 wildlife species in the Snapshot Serengeti dataset, with the highest Top-1 and Top-5 accuracy levels of 95.3% and 98.8%, respectively. Silva et al. [<xref rid="B7-animals-13-00838" ref-type="bibr">7</xref>] utilized ResNet50 to classify different species of &#x0201c;bush pigs&#x0201d;, with a best accuracy of 98.33%. Nguyen et al. [<xref rid="B8-animals-13-00838" ref-type="bibr">8</xref>] designed Lite AlexNet to identify the three most common species in South Central Victoria, Australia, with an accuracy of 90.4%. Tan et al. [<xref rid="B5-animals-13-00838" ref-type="bibr">5</xref>] compared three mainstream detection models, YOLOv5, FCOS, and Cascade R-CNN, on the Northeast Tiger and Leopard National Park wildlife image dataset (NTLNP dataset). YOLOv5, FCOS, and Cascade R-CNN all obtained high average precision values: &#x0003e;97.9% at mAP_0.5 and &#x0003e;81.2% at mAP_0.5: 0.95. These studies indicate that deep convolutional neural networks (CNNs) can perform well in wildlife recognition.</p><p>Although existing automatic wildlife recognition methods have achieved higher and higher accuracy, these models&#x02019; capacity to generalize across diverse datasets is not as strong as it could be. Geirhos et al. [<xref rid="B9-animals-13-00838" ref-type="bibr">9</xref>] suggested that the aforementioned issue might be attributed to shortcut learning, in which these models tend to learn simple decision rules during training. These learned decision rules can only perform well on datasets that are independent and identically distributed (i.i.d.). In a non-independent and identically distributed (non-IID) dataset, however, performance deteriorated. Because camera traps are typically deployed in fixed locations, wildlife monitoring images appear to have similar backgrounds over time. This demonstrates a strong coupling link between wildlife and their backgrounds, providing shortcuts for a deep learning model to recognize wildlife via the backgrounds. These models may fail to recognize the same species with different backgrounds given the shortcut-learned decision rules. To increase the accuracy and generalization capabilities of wildlife recognition models, shortcut learning must be avoided.</p><p>Many strategies for mitigating shortcut learning have been investigated. Szegedy et al. [<xref rid="B10-animals-13-00838" ref-type="bibr">10</xref>] proved that data distribution has a direct impact on deep neural network learning and generated adversarial samples by adding perturbations to the data to avoid shortcut learning. Cubuk et al. [<xref rid="B11-animals-13-00838" ref-type="bibr">11</xref>] improved the generalization of object recognition models by augmenting data with information, geometric distortion, and color distortion. Arjovsky et al. [<xref rid="B12-animals-13-00838" ref-type="bibr">12</xref>] used causality to distinguish the false correlation and the interest region in the sample and then proposed invariant risk minimization (IRM), a novel learning framework that can estimate nonlinear, invariant, causal predictors across different training environments, allowing for out-of-distribution generalization. Finn et al. [<xref rid="B13-animals-13-00838" ref-type="bibr">13</xref>] utilized meta-learning to train a model on multiple learning tasks, resulting in high generalization performance with only a modest number of new training samples. Overall, data augmentation is a a simple and effective technique for avoiding shortcut learning.</p><p>Furthermore, it is beneficial to conduct recognition directly on a camera trap to improve the effectiveness of wildlife monitoring [<xref rid="B14-animals-13-00838" ref-type="bibr">14</xref>]. Due to the limited computing capability and memory of camera traps, a lightweight recognition model is required. Model compression is a commonly used method to generate a lightweight model [<xref rid="B15-animals-13-00838" ref-type="bibr">15</xref>]. Knowledge distillation [<xref rid="B16-animals-13-00838" ref-type="bibr">16</xref>] transfers the knowledge gained by a large teacher network to a small student network without losing validity, allowing model compression to be realized. The structure of the student model is crucial to the success of compression and the performance of the compressed model. Wen et al. [<xref rid="B17-animals-13-00838" ref-type="bibr">17</xref>] suggested a structural sparse learning approach for obtaining the student network from a large CNN, which accelerated AlexNet by 5.1 and 3.1 times on CPU and GPU, respectively, with only a 1% drop in accuracy. Rather than compressing the teacher network, Du et al. [<xref rid="B18-animals-13-00838" ref-type="bibr">18</xref>] selected a shallow reference model as the student network, which was then combined with a random forest model to generate more precise probability values for each class. Crowley et al. [<xref rid="B19-animals-13-00838" ref-type="bibr">19</xref>] separated the normal convolution of the teacher model into different groups of point-wise convolutions to construct several student models with test errors ranging from 5.0% to 7.87%.</p><p>Model pruning is another popular model compression method that can significantly reduce the number of parameters by removing certain parts of the model [<xref rid="B20-animals-13-00838" ref-type="bibr">20</xref>]. It is regarded as an effective method for achieving a student network. There are two types of pruning, i.e., unstructured pruning and structured pruning. Unstructured pruning approaches remove weights on a case-by-case basis, e.g., HashedNet [<xref rid="B20-animals-13-00838" ref-type="bibr">20</xref>], which uses a hash function to randomly group network weights, then allows weights in the same group to share parameters to minimize model complexity. Structured pruning methods remove weights in groups, such as a channel or a layer, and are often more efficient than unstructured pruning methods. Li et al. [<xref rid="B21-animals-13-00838" ref-type="bibr">21</xref>] removed convolution kernels with low effect on network accuracy and then retrained the pruned model to boost accuracy. Luo et al. [<xref rid="B22-animals-13-00838" ref-type="bibr">22</xref>] developed a pruning technique for ThiNet based on a greedy strategy. By considering network pruning to be an optimization issue, the statistical information obtained from the next layer&#x02019;s input&#x02013;output relationship is utilized to determine how to prune the present layer. ThiNet can reduce the parameters and FLOPs of ResNet-50 by more than half, whereas top-5 can only reduce them by 1%. He et al. [<xref rid="B23-animals-13-00838" ref-type="bibr">23</xref>] utilized a channel pruning approach to reduce the channel information in the input feature map before retaining it in the output feature map by adjusting the weights. Under a scenario with five-fold acceleration, the approach only degrades the accuracy of a VGG16 network by 0.3%. To achieve a high compression ratio, Jin et al. [<xref rid="B24-animals-13-00838" ref-type="bibr">24</xref>] proposed a hybrid pruning strategy that integrated kernel pruning and weight pruning. Aghli et al. [<xref rid="B25-animals-13-00838" ref-type="bibr">25</xref>] first employed the weight pruning method to create a student model, then knowledge distillation was achieved by minimizing the cosine similarity between the layers of the teacher and student networks. When compared to the state-of-the-art methods, higher compression rates can be achieved with comparable accuracy. All in all, it appears that the combination method of model pruning and knowledge distillation is more suited for generating a lightweight student network with acceptable accuracy.</p><p>In this paper, we propose a lightweight automatic wildlife recognition model design method that avoids shortcut learning. To the best of our knowledge, this is the first work that focuses on the shortcut learning of camera trap image recognition. First, two data augmentation strategies&#x02014;image synthesis (IS) and regional background suppression (RBS)&#x02014;are introduced in order to prevent the wildlife recognition model from shortcut learning and improve its performance. The Resnet50-based wildlife recognition model is then pruned with the genetic algorithm and adaptive BN (GA-ABN) to construct the student model. Finally, utilizing the Resnet50-based wildlife recognition model as a teacher model, knowledge distillation is employed to fine-tune the student model, yielding a lightweight automatic wildlife recognition model. The technological framework for the design of the lightweight automatic wildlife recognition model is depicted in <xref rid="animals-13-00838-f001" ref-type="fig">Figure 1</xref>.</p><p>To summarize, the contribution of this work is two-fold:<list list-type="simple"><list-item><label>(1)</label><p>We introduce a novel mixed data augmentation method that combines IS and RBS to mitigate shortcut learning in wildlife recognition.</p></list-item><list-item><label>(2)</label><p>We propose an effective model compression strategy based on GA-ABN for adaptively reducing the redundant parameters of a large wildlife recognition model while maintaining accuracy.</p></list-item></list></p></sec><sec id="sec2-animals-13-00838"><title>2. Materials and&#x000a0;Methods</title><sec id="sec2dot1-animals-13-00838"><title>2.1. Wildlife-6&#x000a0;Dataset</title><p>From 2010 to 2019, we utilized infrared camera traps to collect wildlife monitoring images in Saihanwula National Nature Reserve in Inner Mongolia [<xref rid="B26-animals-13-00838" ref-type="bibr">26</xref>]. We selected images of six common species: red deer, goral, roe deer, lynx, badger, and wild boar [<xref rid="B27-animals-13-00838" ref-type="bibr">27</xref>]. After discarding images that were falsely triggered or damaged, the LabelMe software [<xref rid="B28-animals-13-00838" ref-type="bibr">28</xref>] was used to annotate the remaining images with bounding boxes and categories. The annotated dataset is denoted as Wildlife-6. Details of Wildlife-6 are shown in <xref rid="animals-13-00838-t001" ref-type="table">Table 1</xref>.</p></sec><sec id="sec2dot2-animals-13-00838"><title>2.2. Mixed Data&#x000a0;Augmentation</title><p>In this section, a mixed data augmentation method combining IS and RBS is proposed to diversify the background.</p><sec id="sec2dot2dot1-animals-13-00838"><title>2.2.1. Image Synthesis Based on Random&#x000a0;Pasting</title><p>We present an image synthesis method for generating new training samples by randomly pasting wildlife instances into background images of camera traps. <xref rid="animals-13-00838-f002" ref-type="fig">Figure 2</xref> shows the image synthesis procedure. There are two main stages: (1) target segmentation and (2) random pasting.</p><p>In the target segmentation stage, wildlife instances are extracted using the weakly supervised semantic segmentation model Inter-pixel Relation Network (IRNet) [<xref rid="B29-animals-13-00838" ref-type="bibr">29</xref>]. First, we trained the ResNet50-based wildlife recognition model and generated the class activation mapping (CAM). The confidence region of CAM was then obtained using the DenseCFR method, which was based on the thresholding of the foreground and background. The random walk method was applied to select pairs of points in the confidence region, which were then input into the IRNet for training. During training, IRNet learns to produce class boundary maps to determine class boundaries and compute class centroids. The optimal class centroids are achieved by iteration. Finally, the random walk algorithm is used to compute the attention scores of the pixel points in the CAM, as well as the domain propagation of the instance image based on the semantic affinity between adjacent pixels, to obtain the entire instance region, i.e., the wildlife instance.</p><p>Furthermore, in the random pasting stage, the extracted wildlife instances are randomly rotated, resized, and then pasted to a random background image to disrupt their inherent directional properties and reduce the object&#x02019;s dependency on the entrenched scene. <xref rid="animals-13-00838-f003" ref-type="fig">Figure 3</xref> depicts synthetic sample instances.</p></sec><sec id="sec2dot2dot2-animals-13-00838"><title>2.2.2. Regional Background&#x000a0;Suppression</title><p>We propose a target-guided background suppression method to guide the network to focus on the foreground (wildlife). Unlike the cutout method, our method only randomly suppresses the background regions. The background suppression is divided into the following stages.</p><p>To begin, separate the image into foreground (wildlife) and background. To properly extract the foreground, we label the wildlife in the image with a bounding box, and the region outside the bounding box is regarded as background.</p><p>Then, given a random starting point and height&#x02013;width ratio, we generate a rectangular mask <inline-formula><mml:math id="mm1" overflow="scroll"><mml:mrow><mml:msub><mml:mi>m</mml:mi><mml:mi>b</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> with all values equal to 0. With the bounding box region <inline-formula><mml:math id="mm2" overflow="scroll"><mml:mrow><mml:msub><mml:mi>m</mml:mi><mml:mi>f</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula>, it is critical to guarantee that <inline-formula><mml:math id="mm3" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi>m</mml:mi><mml:mi>f</mml:mi></mml:msub><mml:mo>&#x02229;</mml:mo><mml:msub><mml:mi>m</mml:mi><mml:mi>b</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mo>&#x02205;</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>. In this case, the random mask suppresses background features, forcing the network to focus on foreground (wildlife) features. The generated images with background occlusion may be utilized not only to model occlusion phenomena in the wild, but also to help the model focus on more foreground (wildlife) information.</p></sec></sec><sec id="sec2dot3-animals-13-00838"><title>2.3. Lightweight Method for Automatic Wildlife Recognition&#x000a0;Model</title><sec id="sec2dot3dot1-animals-13-00838"><title>2.3.1. Pruning Method Based on Genetic Algorithm and Adaptive Batch&#x000a0;Normalization</title><p>A structural prune method based on GA-ABN is presented to construct the student model. First, the genetic algorithm(GA) is applied to obtain an optimal pruning strategy. To begin, a set of sub-networks are created as the initial population using a random sampling method. A fitness function is introduced to evaluate each sub-network in the initial population and is shown in Equation (<xref rid="FD1-animals-13-00838" ref-type="disp-formula">1</xref>).
<disp-formula id="FD1-animals-13-00838"><label>(1)</label><mml:math id="mm4" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi>f</mml:mi><mml:mi>j</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mfrac><mml:msub><mml:mi>n</mml:mi><mml:mi>j</mml:mi></mml:msub><mml:mrow><mml:msub><mml:mi>p</mml:mi><mml:mi>j</mml:mi></mml:msub><mml:mo>/</mml:mo><mml:msub><mml:mi>p</mml:mi><mml:mn>0</mml:mn></mml:msub></mml:mrow></mml:mfrac></mml:mrow></mml:mrow></mml:math></disp-formula>
where <inline-formula><mml:math id="mm5" overflow="scroll"><mml:mrow><mml:msub><mml:mi>n</mml:mi><mml:mi>j</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> is the validation accuracy of the <italic toggle="yes">j</italic>-th sub-network, and the parameters <inline-formula><mml:math id="mm6" overflow="scroll"><mml:mrow><mml:msub><mml:mi>p</mml:mi><mml:mi>j</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="mm7" overflow="scroll"><mml:mrow><mml:msub><mml:mi>p</mml:mi><mml:mn>0</mml:mn></mml:msub></mml:mrow></mml:math></inline-formula> represent the number of the <italic toggle="yes">j</italic>-th sub-network and the initial network, respectively.</p><p>The fitness score of each sub-network is then determined in the selection stage. The top 20 sub-networks with the highest fitness scores are chosen. During the crossover mutation step, a new population with 100 new sub-networks is generated based on the 20 previously selected sub-networks. The selection and mutation steps are repeated until the total fitness of the new population remains constant. Finally, an optimal pruned network can be achieved.</p><p>Furthermore, because the commonly used global BN may result in sub-optimal performance [<xref rid="B30-animals-13-00838" ref-type="bibr">30</xref>], we updated the BN [<xref rid="B31-animals-13-00838" ref-type="bibr">31</xref>] to an adaptive BN to overcome this issue. For mini-batch samples of size <italic toggle="yes">N</italic>, the training data <inline-formula><mml:math id="mm8" overflow="scroll"><mml:mrow><mml:msub><mml:mi>x</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula>, the statistics of <inline-formula><mml:math id="mm9" overflow="scroll"><mml:mrow><mml:mi>&#x003bc;</mml:mi></mml:mrow></mml:math></inline-formula>, and <inline-formula><mml:math id="mm10" overflow="scroll"><mml:mrow><mml:msup><mml:mi>&#x003c3;</mml:mi><mml:mn>2</mml:mn></mml:msup></mml:mrow></mml:math></inline-formula> used in the adaptive BN are computed with Equations (<xref rid="FD2-animals-13-00838" ref-type="disp-formula">2</xref>) and (<xref rid="FD3-animals-13-00838" ref-type="disp-formula">3</xref>).
<disp-formula id="FD2-animals-13-00838"><label>(2)</label><mml:math id="mm11" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:msub><mml:mi>&#x003bc;</mml:mi><mml:mi>B</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mi>N</mml:mi></mml:mfrac><mml:munderover><mml:mo>&#x02211;</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>N</mml:mi></mml:munderover><mml:msub><mml:mi>x</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:mrow></mml:math></disp-formula>
<disp-formula id="FD3-animals-13-00838"><label>(3)</label><mml:math id="mm12" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:msubsup><mml:mi>&#x003c3;</mml:mi><mml:mrow><mml:mi>B</mml:mi></mml:mrow><mml:mn>2</mml:mn></mml:msubsup><mml:mo>=</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mrow><mml:mi>N</mml:mi><mml:mo>&#x02212;</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:mfrac><mml:munderover><mml:mo>&#x02211;</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>N</mml:mi></mml:munderover><mml:msup><mml:mfenced separators="" open="(" close=")"><mml:msub><mml:mi>x</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>&#x02212;</mml:mo><mml:msub><mml:mi>&#x003bc;</mml:mi><mml:mi>B</mml:mi></mml:msub></mml:mfenced><mml:mn>2</mml:mn></mml:msup></mml:mrow></mml:mrow></mml:math></disp-formula></p></sec><sec id="sec2dot3dot2-animals-13-00838"><title>2.3.2. Fine-Tuning Based on Knowledge Distillation with MSE&#x000a0;Loss</title><p>Knowledge distillation is introduced to improve the accuracy of the student model. As shown in <xref rid="animals-13-00838-f004" ref-type="fig">Figure 4</xref>, we jointly train the teacher model and the student model with knowledge distillation loss, resulting in the student model&#x02019;s distribution being comparable to that of the teacher model.</p><p>The knowledge distillation with MSE loss is introduced to fine-tune the pruned model (see <xref rid="animals-13-00838-f004" ref-type="fig">Figure 4</xref>a). The MSE loss [<xref rid="B32-animals-13-00838" ref-type="bibr">32</xref>] is applied to compute the difference between the output distributions of the teacher and student models. For comparison, two types of KL divergence-based knowledge distillation (cf. <xref rid="animals-13-00838-f004" ref-type="fig">Figure 4</xref>b) are included: knowledge distillation with soft KL (KD-SKL) and knowledge distillation with hard KL (KD-HKL). In KL divergence-based knowledge distillation, the teacher model imposes a supervision signal on the student model. KD-SKL adopts probability scores as a supervision signal, whereas KD-HKL uses one-hot pseudo labels, resulting in two different KL divergence losses.</p><p>Following knowledge distillation, the fine-tuned student model may be viewed as a lightweight wildlife recognition model with relatively high performance.</p></sec></sec></sec><sec id="sec3-animals-13-00838"><title>3. Experiments and&#x000a0;Results</title><sec id="sec3dot1-animals-13-00838"><title>3.1. Experiments&#x000a0;Setup</title><p>Because ResNet-50 provides excellent performance in wildlife image recognition [<xref rid="B6-animals-13-00838" ref-type="bibr">6</xref>], we built the ResNet50-based wildlife recognition model as the teacher model. During training, the dataset was divided into training and test sets in a 7:3 ratio. All images were downsized to <inline-formula><mml:math id="mm13" overflow="scroll"><mml:mrow><mml:mrow><mml:mn>448</mml:mn><mml:mo>&#x000d7;</mml:mo><mml:mn>448</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula> before being fed to the model.</p><p><xref rid="animals-13-00838-t002" ref-type="table">Table 2</xref> shows the software and hardware used in all experiments. All of the models&#x02019; main training settings are the same, as indicated in <xref rid="animals-13-00838-t003" ref-type="table">Table 3</xref>.</p></sec><sec id="sec3dot2-animals-13-00838"><title>3.2. Evaluation&#x000a0;Metrics</title><p>The following metrics are introduced to evaluate the performance of the models:<list list-type="simple"><list-item><label>(1)</label><p>Classification performance</p></list-item></list></p><p>The accuracy calculated by Equation (<xref rid="FD4-animals-13-00838" ref-type="disp-formula">4</xref>) is used to evaluate the classification performance.
<disp-formula id="FD4-animals-13-00838"><label>(4)</label><mml:math id="mm14" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>A</mml:mi><mml:mi>C</mml:mi><mml:msub><mml:mi>C</mml:mi><mml:mrow><mml:mi>c</mml:mi><mml:mi>l</mml:mi><mml:mi>s</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:msubsup><mml:mo>&#x02211;</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>c</mml:mi></mml:msubsup><mml:mi>T</mml:mi><mml:msub><mml:mi>P</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow><mml:mrow><mml:msubsup><mml:mo>&#x02211;</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>c</mml:mi></mml:msubsup><mml:mfenced separators="" open="(" close=")"><mml:mi>T</mml:mi><mml:msub><mml:mi>P</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>+</mml:mo><mml:mi>F</mml:mi><mml:msub><mml:mi>P</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mfenced></mml:mrow></mml:mfrac></mml:mrow></mml:mrow></mml:math></disp-formula>
where <inline-formula><mml:math id="mm15" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>T</mml:mi><mml:msub><mml:mi>P</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula> (true positive) represents the number of correctly classified images of the <italic toggle="yes">i</italic>th class and <inline-formula><mml:math id="mm16" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>F</mml:mi><mml:msub><mml:mi>P</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula> (false positive) represents the number of misclassified images of the <italic toggle="yes">i</italic>th class. <italic toggle="yes">C</italic> is the total number of categories.</p><list list-type="simple"><list-item><label>(2)</label><p>Performance of mitigating shortcut learning</p></list-item></list><p>A heatmap can indicate where a model focuses on during classification [<xref rid="B33-animals-13-00838" ref-type="bibr">33</xref>]. We propose the foreground ratio of the heatmap (FRoH) to evaluate the performance of mitigating shortcut learning. The FRoH is defined by Equation (<xref rid="FD5-animals-13-00838" ref-type="disp-formula">5</xref>):<disp-formula id="FD5-animals-13-00838"><label>(5)</label><mml:math id="mm17" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>F</mml:mi><mml:mi>R</mml:mi><mml:mi>o</mml:mi><mml:mi>H</mml:mi><mml:mo>=</mml:mo><mml:mfrac><mml:msub><mml:mi>H</mml:mi><mml:mrow><mml:mi>box</mml:mi><mml:mspace width="4.pt"/></mml:mrow></mml:msub><mml:msub><mml:mi>H</mml:mi><mml:mrow><mml:mi>total</mml:mi><mml:mspace width="4.pt"/></mml:mrow></mml:msub></mml:mfrac></mml:mrow></mml:mrow></mml:math></disp-formula>
where <inline-formula><mml:math id="mm18" overflow="scroll"><mml:mrow><mml:msub><mml:mi>H</mml:mi><mml:mrow><mml:mi>total</mml:mi><mml:mspace width="4.pt"/></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula> is the sum of the thermal values of all pixels in the heatmap, and <inline-formula><mml:math id="mm19" overflow="scroll"><mml:mrow><mml:msub><mml:mi>H</mml:mi><mml:mrow><mml:mi>box</mml:mi><mml:mspace width="4.pt"/></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula> is the sum of the thermal values of the pixels in the labeled bounding box. <xref rid="animals-13-00838-f005" ref-type="fig">Figure 5</xref> shows an intuitive schematic diagram of FRoH. FRoH indicates how much the model focuses on the targets (wildlife). The higher the FRoH value, the more the model is focused on the wildlife, implying that the model classifies images primarily on wildlife features rather than background and that shortcut learning is mitigated.</p><list list-type="simple"><list-item><label>(3)</label><p>Performance of model compression</p></list-item></list><p>The number of parameters, number of calculations, and FPS (frames per second) are introduced to quantify the performance of model compression. Equations (<xref rid="FD6-animals-13-00838" ref-type="disp-formula">6</xref>) and (<xref rid="FD7-animals-13-00838" ref-type="disp-formula">7</xref>) specify the number of parameters and number of calculations of the convolutional layer, respectively.
<disp-formula id="FD6-animals-13-00838"><label>(6)</label><mml:math id="mm20" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:mspace width="4.pt"/><mml:mi>params</mml:mi><mml:mspace width="4.pt"/><mml:mo>=</mml:mo><mml:msub><mml:mi>c</mml:mi><mml:mrow><mml:mi>out</mml:mi><mml:mspace width="4.pt"/></mml:mrow></mml:msub><mml:mo>&#x000d7;</mml:mo><mml:mfenced separators="" open="(" close=")"><mml:msup><mml:mi>K</mml:mi><mml:mn>2</mml:mn></mml:msup><mml:mo>&#x000d7;</mml:mo><mml:msub><mml:mi>c</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>n</mml:mi></mml:mrow></mml:msub></mml:mfenced></mml:mrow></mml:mrow></mml:math></disp-formula>
<disp-formula id="FD7-animals-13-00838"><label>(7)</label><mml:math id="mm21" display="block" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>FLOPs</mml:mi><mml:mo>=</mml:mo><mml:mn>2</mml:mn><mml:mi>H</mml:mi><mml:mi>W</mml:mi><mml:mfenced separators="" open="(" close=")"><mml:msub><mml:mi>c</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>n</mml:mi></mml:mrow></mml:msub><mml:msup><mml:mi>K</mml:mi><mml:mn>2</mml:mn></mml:msup><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mfenced><mml:msub><mml:mi>c</mml:mi><mml:mrow><mml:mi>o</mml:mi><mml:mi>u</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math></disp-formula>
where <inline-formula><mml:math id="mm22" overflow="scroll"><mml:mrow><mml:msub><mml:mi>C</mml:mi><mml:mrow><mml:mi>out</mml:mi><mml:mspace width="4.pt"/></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="mm23" overflow="scroll"><mml:mrow><mml:msub><mml:mi>C</mml:mi><mml:mrow><mml:mi>in</mml:mi><mml:mspace width="4.pt"/></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula> represent the number of output and input channels, respectively. <italic toggle="yes">H</italic> and <italic toggle="yes">W</italic> represent the height and width of the input feature map, respectively. <italic toggle="yes">K</italic> is the size of the convolution kernel.</p></sec><sec id="sec3dot3-animals-13-00838"><title>3.3. Experiments Results of Mitigating Shortcut Learning</title><sec id="sec3dot3dot1-animals-13-00838"><title>3.3.1. Comparison Experiments with Different Data Augmentation Methods</title><p>Using the ResNet50-based wildlife recognition model as a baseline, five different data augmentation methods were compared, including repeat sampling, cutout [<xref rid="B34-animals-13-00838" ref-type="bibr">34</xref>], IS, RBS and the mixed method. The results shown in <xref rid="animals-13-00838-t004" ref-type="table">Table 4</xref> indicate that our proposed method achieves the best performance, with <inline-formula><mml:math id="mm24" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>A</mml:mi><mml:mi>C</mml:mi><mml:msub><mml:mi>C</mml:mi><mml:mrow><mml:mi>c</mml:mi><mml:mi>l</mml:mi><mml:mi>s</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula> and FRoH values of 91.23% and 44.71%, respectively. Surprisingly, the results using repeat sampling are somewhat lower than the baseline. This is because simple random repeat sampling has no effect on the overall distribution of the dataset. The strong coupling between wildlife and background remains, resulting in no improvement in recognition accuracy. The cutout method has a higher FRoH score than the baseline, implying that the cutout method can decrease the coupling between wildlife and background to some extent. However, the cutout method has a slightly lower <inline-formula><mml:math id="mm25" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>A</mml:mi><mml:mi>C</mml:mi><mml:msub><mml:mi>C</mml:mi><mml:mrow><mml:mi>c</mml:mi><mml:mi>l</mml:mi><mml:mi>s</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula> value than that of the baseline. Because the cutout method&#x02019;s optional suppression regions cover the whole image, including the wildlife region, the wildlife feature may be lost. Both IS and RBS perform better than the baseline. IS outperforms RBS because it actively diversifies the background, resulting in more abundant variety than simply suppressing a portion of the background. As expected, the mixed data augmentation method outperforms IS and RBS.</p></sec><sec id="sec3dot3dot2-animals-13-00838"><title>3.3.2. Visualization&#x000a0;Analysis</title><p>To conduct a more in-depth analysis of the performance improvement brought by the mixed data augmentation method, we calculated a heatmap using gradient-weighted class activation mapping (Grad-CAM) [<xref rid="B35-animals-13-00838" ref-type="bibr">35</xref>]. Heatmaps of example images with or without mixed data augmentation method are illustrated in <xref rid="animals-13-00838-f006" ref-type="fig">Figure 6</xref>. It can be observed that with mixed data augmentation, the trained wildlife recognition model can precisely focus on the wildlife (cf. last row in <xref rid="animals-13-00838-f006" ref-type="fig">Figure 6</xref>). However, in the absence of mixed data augmentation, the trained wildlife recognition model is easily disturbed by the background or can only acquire incomplete wildlife features (see the second row in <xref rid="animals-13-00838-f006" ref-type="fig">Figure 6</xref>).</p><p><xref rid="animals-13-00838-f007" ref-type="fig">Figure 7</xref> shows the recognition results for example images of gorals, badgers, red deer, and roe deer. Without the mixed data augmentation, the wildlife recognition model predicted the wrong categories of wildlife (see the second column in <xref rid="animals-13-00838-f007" ref-type="fig">Figure 7</xref>), which can be ascribed to an excessive focus on the background. The wildlife recognition model with the mixed data augmentation method is completely concerned with all wildlife, resulting in accurate predictions (see the third column in <xref rid="animals-13-00838-f007" ref-type="fig">Figure 7</xref>).</p></sec><sec id="sec3dot3dot3-animals-13-00838"><title>3.3.3. Class-Wise&#x000a0;Accuracy</title><p>Confusion matrices were computed to investigate the effect of the proposed mixed data augmentation strategy on the recognition performance of different species, as shown in <xref rid="animals-13-00838-f008" ref-type="fig">Figure 8</xref>. When comparing <xref rid="animals-13-00838-f008" ref-type="fig">Figure 8</xref>a,b, the mixed data augmentation strategy improved the recognition accuracy of all species. Lynxes and badgers, in particular, improved by 7.3% and 6.1%, respectively. The limited sample size and background type of these two species, together with their small size, made it easier for the model to accomplish recognition by background, and so enrichment of background variety resulted in a larger improvement.</p></sec><sec id="sec3dot3dot4-animals-13-00838"><title>3.3.4. Performance on Other&#x000a0;Dataset</title><p>We investigated the mixed data augmentation strategy further on the NACTI dataset. The NACTI dataset contains around 3.7 million camera trap monitoring images from five locations across the United States. We chose eleven species for comparative experiments. As with the Wildlife-6 dataset, the bounding boxes containing wildlife were annotated and used in the following experiments. <xref rid="animals-13-00838-t005" ref-type="table">Table 5</xref> shows the results on the NACTI dataset. The <inline-formula><mml:math id="mm26" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>A</mml:mi><mml:mi>C</mml:mi><mml:msub><mml:mi>C</mml:mi><mml:mrow><mml:mi>c</mml:mi><mml:mi>l</mml:mi><mml:mi>s</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula> and FRoH values were both enhanced after utilizing the mixed data augmentation method, with the FRoH being increased to a greater extent. The improvement in <inline-formula><mml:math id="mm27" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>A</mml:mi><mml:mi>C</mml:mi><mml:msub><mml:mi>C</mml:mi><mml:mrow><mml:mi>c</mml:mi><mml:mi>l</mml:mi><mml:mi>s</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula> is rather minimal due to the excellent accuracy of the baseline and the more diverse backgrounds in the NACTI dataset.</p><p>In summary, the mixed data augmentation strategy can assist the wildlife recognition model in focusing more on the wildlife, resulting in higher classification accuracy and generalization across datasets. In other words, the shortcut learning can be mitigated.</p></sec></sec><sec id="sec3dot4-animals-13-00838"><title>3.4. Experiments Results of the Lightweight Automatic Recognition&#x000a0;Model</title><sec id="sec3dot4dot1-animals-13-00838"><title>3.4.1. Comparison with Different Pruning&#x000a0;Method</title><p>With the ResNet50-based wildlife recognition model as the teacher model, two pruning methods, the GA-ABN and random sampling algorithms, were compared. The compression ratio of FLOPs was set to 50 &#x000b1; 5%. The fitness variations of the sub-networks generated by two pruning methods during the searching process are depicted in <xref rid="animals-13-00838-f009" ref-type="fig">Figure 9</xref>. It can be shown that the fitness (i.e., accuracy) of the random sampling method improved significantly at first, but then almost equalized in the later search period. This is due to the lack of an optimization mechanism. Taking advantage of GA&#x02019;s evolution mechanism, our method can achieve higher and higher fitness as time passes, implying a better pruning strategy. Meanwhile, the final fitness of our method is greater than that of the pruning method using random sampling.</p><p>We further fine-tuned the optimal pruned model obtained by two pruning methods. As shown in <xref rid="animals-13-00838-t006" ref-type="table">Table 6</xref>, the accuracy of the ResNet50-based model is 91.23%, the number of parameters is 23.52 M, the FLOPs is 16.48 G and the FPS is 0.85. The number of parameters of the two pruned models were reduced by almost half. Furthermore, the accuracy values of the two pruned models were decreased as expected, and the pruned model by GA-ABN achieved greater accuracy with fewer parameters than the pruned model by random sampling. Similar to the numbers of parameters, the FLOPs of both pruned models declined by almost half, resulting in higher FPS. The FPS values of both pruned models increased by nearly 62 times as compared with the ResNet50 model. Although the FLOP and FPS of our method are similar with those of random sampling, our method has a higher accuracy than random sampling.</p><p>Furthermore, the compression ratio of FLOPs was set to 25 &#x000b1; 5% to verify the generalization of the proposed pruning method. As shown in <xref rid="animals-13-00838-t007" ref-type="table">Table 7</xref>, although FLOPs and FPS are the same, the accuracy of the pruned model achieved by our method is still higher than that of the pruned model using random sampling.</p><p>Altogether, the pruning method using GA-ABN can effectively reduce the number of computations in the model, speed up the model operation, and maintain relatively high accuracy under different pruning ratios, showing that the GA-ABN pruning method has excellent compression performance.</p></sec><sec id="sec3dot4dot2-animals-13-00838"><title>3.4.2. Fine-Tuning Based on Different Knowledge Distillation&#x000a0;Methods</title><p>Three fine-tuning strategies based on KD-MSE, KD-SKL, and KD-HKL were compared with the pruned ResNet50 model using GA-ABN as the baseline model (P-ResNet50). As shown in <xref rid="animals-13-00838-t008" ref-type="table">Table 8</xref>, all three fine-tuning strategies outperformed the P-ResNet50 model in terms of accuracy. Furthermore, the accuracy of KD-MSE was 1.88%, 0.66%, and 1.39% higher than those of P-ResNet50, KD-SKL, and KD-HKL, respectively, indicating that KD-MSE is the most effective fine-tuning method. The advantage of KD-MSE originates from the fact that it enables the student network to inherit the output distribution of the teacher network via an explicit MSE constraint. KD-HKL, in particular, performs worse than KD-SKL, which we attribute to possible noise introduced by a hard supervision signal during training.</p></sec></sec></sec><sec sec-type="discussion" id="sec4-animals-13-00838"><title>4. Discussion</title><p>Wildlife camera trap image recognition based on deep learning offers enormous potential, but it is also challenging due to the complexity of the wild environment [<xref rid="B3-animals-13-00838" ref-type="bibr">3</xref>]. Our work investigates a mixed data augmentation technique for mitigating shortcut learning of deep learning models caused by similar backgrounds in certain images from the same camera trap. Intelligent recognition implemented directly on edge devices can improve the effectiveness of wildlife monitoring [<xref rid="B36-animals-13-00838" ref-type="bibr">36</xref>], we further propose a novel model compression strategy that integrates pruning and knowledge distillation to develop lightweight wildlife recognition models applicable to camera traps.</p><sec id="sec4dot1-animals-13-00838"><title>4.1. Overcoming Shortcut Learning with Data&#x000a0;Augmentation</title><p>Overcoming shortcut learning can enhance model generalization performance. However, to the best of the authors&#x02019; knowledge, no work has focused on shortcut learning in camera trap image recognition. Because dataset distribution has a direct influence on deep neural network learning, data distribution variation has the potential to alleviate shortcut learning [<xref rid="B10-animals-13-00838" ref-type="bibr">10</xref>]. Several data augmentation strategies, which can vary the data distribution, have been employed to alleviate shortcut learning [<xref rid="B11-animals-13-00838" ref-type="bibr">11</xref>].</p><p>Diversifying the background in camera trap images can help distinguish the foreground (wildlife) and background and prevent the recognition model from shortcut learning. Given that repeat sampling has no effect on data distribution, it is therefore not surprising that the recognition performance has not been improved. The cutout method modifies the data distribution using random masks and improves the FRoH score compared to the baseline, but it may also suppress the foreground (wildlife) feature, which lowers the <inline-formula><mml:math id="mm28" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>A</mml:mi><mml:mi>C</mml:mi><mml:msub><mml:mi>C</mml:mi><mml:mrow><mml:mi>c</mml:mi><mml:mi>l</mml:mi><mml:mi>s</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula> in comparison to the baseline. IS and RBS both retain foreground features while modifying the data distribution, achieve higher FRoHs than the baseline, guide the model to focus more on the foreground, and avoid shortcut learning very well. As expected, a mixed data augmentation approach that combines IS and RBS performs the best. Using the Wildlife-6 dataset, our recognition accuracy (91.23%) is higher than that of the literature [<xref rid="B6-animals-13-00838" ref-type="bibr">6</xref>] (90.2%), demonstrating that our technique is effective.</p></sec><sec id="sec4dot2-animals-13-00838"><title>4.2. Lightweight Wildlife Recognition&#x000a0;Model</title><p>Because camera traps have limited computational capacity and memory, lightweight models are required for deep learning-based automatic wildlife recognition in camera traps [<xref rid="B36-animals-13-00838" ref-type="bibr">36</xref>]. Model compression strategies based on model pruning and knowledge distillation are the dominant approaches for developing lightweight models [<xref rid="B37-animals-13-00838" ref-type="bibr">37</xref>,<xref rid="B38-animals-13-00838" ref-type="bibr">38</xref>]. In knowledge distillation methods, selecting a suitable student model is crucial. Model pruning can produce beneficial tiny networks, but it can also reduce accuracy [<xref rid="B20-animals-13-00838" ref-type="bibr">20</xref>,<xref rid="B21-animals-13-00838" ref-type="bibr">21</xref>]. A potential strategy appears to be model pruning to generate student models and knowledge distillation for fine-tuning [<xref rid="B25-animals-13-00838" ref-type="bibr">25</xref>].</p><p>The key to structured pruning is to search for redundant channels or layers, which is an optimization problem. GA can effectively avoid falling into local optima and has been utilized in model pruning [<xref rid="B38-animals-13-00838" ref-type="bibr">38</xref>]. Experiments show that the proposed GA-ABN can search for and achieve sub-networks with higher final fitness scores more quickly when compared to random sampling algorithms. The GA-ABN method can achieve a higher accuracy than random sampling algorithms. The greater the compression rate, the more obvious the benefit. With a compression rate of 50 &#x000b1; 5%, the FPS of our method reaches 53.49, about 63 times that of ResNet50, which is sufficient for provision of real-time results (30 FPS or more) [<xref rid="B39-animals-13-00838" ref-type="bibr">39</xref>].</p><p>Furthermore, we introduced three kinds of knowledge distillation methods, KD-MSE, KD-SKL and KD-HKL to fine-tune the pruned model to improve accuracy. MSE loss can make the student model learn the output distribution of the teacher model more intuitively [<xref rid="B40-animals-13-00838" ref-type="bibr">40</xref>], which makes the KD-MSE achieved the highest accuracy improvement among three methods.</p><p>Finally, a lightweight wildlife recognition model with high accuracy but a massively reduced number of parameters and a significantly lower calculation can be achieved. The proposed lightweight wildlife recognition model design method has the potential to enable the application of edge intelligence. Future research will investigate the lightweight model&#x02019;s performance on embedded devices.</p></sec></sec><sec sec-type="conclusions" id="sec5-animals-13-00838"><title>5. Conclusions</title><p>This study proposed a lightweight wildlife recognition model design method that combines a mixed data augmentation strategy, as well as a model compression method with integrated pruning and knowledge distillation.</p><p>The mixed data augmentation method, combined with IS and RBS, was introduced to modify the data distribution of the dataset, guiding the model to focus more on the wildlife feature. The <inline-formula><mml:math id="mm29" overflow="scroll"><mml:mrow><mml:mrow><mml:mi>A</mml:mi><mml:mi>C</mml:mi><mml:mi>C</mml:mi><mml:mi>c</mml:mi><mml:mi>l</mml:mi><mml:mi>s</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula> and FRoH were both improved after using the mixed data augmentation method, demonstrating that shortcut learning was mitigated.</p><p>Furthermore, the lightweight model was constructed using an integrated method that first employed the structural pruning technique based on GA-ABN, then fine-tuned the pruned model with KD-MSE. Using GA-ABN pruning and a compression rate of 50 &#x000b1; 5%, the number of model parameters reduced by 57.4%, FLOPs decreased by 46.1%, FPS improved to 62.9 times, and accuracy decreased by just 4.73%. After fine-tuning with KD-MSE, the accuracy of the model improved by 2.12%.</p><p>Overall, this study provides a novel method for developing lightweight wildlife recognition models, which can result in lightweight models with relatively high accuracy and significantly decreased computing cost. Our method has important practical implications for utilizing deep learning models for wildlife monitoring on edge intelligent devices. This can help to support long-term wildlife monitoring, biodiversity resource assessments, and ecological conservation.</p><p>In the future, we intend to deploy our lightweight model to camera traps and augment them with wireless communication capabilities to create a real-time wildlife monitoring system. The performance of proposed system will be evaluated in the wild.</p></sec></body><back><fn-group><fn><p><bold>Disclaimer/Publisher&#x02019;s Note:</bold> The statements, opinions and data contained in all publications are solely those of the individual author(s) and contributor(s) and not of MDPI and/or the editor(s). MDPI and/or the editor(s) disclaim responsibility for any injury to people or property resulting from any ideas, methods, instructions or products referred to in the content.</p></fn></fn-group><notes><title>Author Contributions</title><p>Conceptualization, J.X. and X.L.; methodology, Y.Z. and X.L.; software, Y.Z. and X.L.; validation, Y.Z. and X.L.; formal analysis, Y.Z. and X.L.; investigation, X.L.; data curation, Y.Z.; writing&#x02014;original draft preparation, Y.Z. and X.L.; writing&#x02014;review and editing, J.X., Y.Z. and J.Z.; supervision, J.X.; project administration, J.X. and J.Z.; funding acquisition, J.X. and J.Z.; All authors have read and agreed to the published version of the manuscript.</p></notes><notes><title>Institutional Review Board Statement</title><p>Not applicable.</p></notes><notes><title>Informed Consent Statement</title><p>Not applicable.</p></notes><notes notes-type="data-availability"><title>Data Availability Statement</title><p>The datasets used in this study are available from the corresponding author on reasonable request.</p></notes><notes notes-type="COI-statement"><title>Conflicts of Interest</title><p>The authors declare no conflict of interest.</p></notes><ref-list><title>References</title><ref id="B1-animals-13-00838"><label>1.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Cremonesi</surname><given-names>G.</given-names></name>
<name><surname>Bisi</surname><given-names>F.</given-names></name>
<name><surname>Gaffi</surname><given-names>L.</given-names></name>
<name><surname>Zaw</surname><given-names>T.</given-names></name>
<name><surname>Naing</surname><given-names>H.</given-names></name>
<name><surname>Moe</surname><given-names>K.</given-names></name>
<name><surname>Aung</surname><given-names>Z.</given-names></name>
<name><surname>Mazzamuto</surname><given-names>M.V.</given-names></name>
<name><surname>Gagliardi</surname><given-names>A.</given-names></name>
<name><surname>Wauters</surname><given-names>L.A.</given-names></name>
<etal/>
</person-group><article-title>Camera trapping to assess status and composition of mammal communities in a biodiversity hotspot in Myanmar</article-title><source>Animals</source><year>2021</year><volume>11</volume><elocation-id>880</elocation-id><pub-id pub-id-type="doi">10.3390/ani11030880</pub-id><pub-id pub-id-type="pmid">33808844</pub-id></element-citation></ref><ref id="B2-animals-13-00838"><label>2.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Tuia</surname><given-names>D.</given-names></name>
<name><surname>Kellenberger</surname><given-names>B.</given-names></name>
<name><surname>Beery</surname><given-names>S.</given-names></name>
<name><surname>Costelloe</surname><given-names>B.R.</given-names></name>
<name><surname>Zuffi</surname><given-names>S.</given-names></name>
<name><surname>Risse</surname><given-names>B.</given-names></name>
<name><surname>Mathis</surname><given-names>A.</given-names></name>
<name><surname>Mathis</surname><given-names>M.W.</given-names></name>
<name><surname>van Langevelde</surname><given-names>F.</given-names></name>
<name><surname>Burghardt</surname><given-names>T.</given-names></name>
<etal/>
</person-group><article-title>Perspectives in machine learning for wildlife conservation</article-title><source>Nat. Commun.</source><year>2022</year><volume>13</volume><fpage>792</fpage><pub-id pub-id-type="doi">10.1038/s41467-022-27980-y</pub-id><pub-id pub-id-type="pmid">35140206</pub-id></element-citation></ref><ref id="B3-animals-13-00838"><label>3.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Leorna</surname><given-names>S.</given-names></name>
<name><surname>Brinkman</surname><given-names>T.</given-names></name>
</person-group><article-title>Human vs. machine: Detecting wildlife in camera trap images</article-title><source>Ecol. Inform.</source><year>2022</year><volume>72</volume><fpage>101876</fpage><pub-id pub-id-type="doi">10.1016/j.ecoinf.2022.101876</pub-id></element-citation></ref><ref id="B4-animals-13-00838"><label>4.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Zhu</surname><given-names>H.</given-names></name>
<name><surname>Tian</surname><given-names>Y.</given-names></name>
<name><surname>Zhang</surname><given-names>J.</given-names></name>
</person-group><article-title>Class incremental learning for wildlife biodiversity monitoring in camera trap images</article-title><source>Ecol. Inform.</source><year>2022</year><volume>71</volume><fpage>101760</fpage><pub-id pub-id-type="doi">10.1016/j.ecoinf.2022.101760</pub-id></element-citation></ref><ref id="B5-animals-13-00838"><label>5.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Tan</surname><given-names>M.</given-names></name>
<name><surname>Chao</surname><given-names>W.</given-names></name>
<name><surname>Cheng</surname><given-names>J.K.</given-names></name>
<name><surname>Zhou</surname><given-names>M.</given-names></name>
<name><surname>Ma</surname><given-names>Y.</given-names></name>
<name><surname>Jiang</surname><given-names>X.</given-names></name>
<name><surname>Ge</surname><given-names>J.</given-names></name>
<name><surname>Yu</surname><given-names>L.</given-names></name>
<name><surname>Feng</surname><given-names>L.</given-names></name>
</person-group><article-title>Animal Detection and Classification from Camera Trap Images Using Different Mainstream Object Detection Architectures</article-title><source>Animals</source><year>2022</year><volume>12</volume><elocation-id>1976</elocation-id><pub-id pub-id-type="doi">10.3390/ani12151976</pub-id><pub-id pub-id-type="pmid">35953964</pub-id></element-citation></ref><ref id="B6-animals-13-00838"><label>6.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Xie</surname><given-names>J.</given-names></name>
<name><surname>Li</surname><given-names>A.</given-names></name>
<name><surname>Zhang</surname><given-names>J.</given-names></name>
<name><surname>Cheng</surname><given-names>Z.</given-names></name>
</person-group><article-title>An integrated wildlife recognition model based on multi-branch aggregation and squeeze-and-excitation network</article-title><source>Appl. Sci.</source><year>2019</year><volume>9</volume><elocation-id>2794</elocation-id><pub-id pub-id-type="doi">10.3390/app9142794</pub-id></element-citation></ref><ref id="B7-animals-13-00838"><label>7.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Silva</surname><given-names>L.C.</given-names></name>
<name><surname>P&#x000e1;dua</surname><given-names>M.B.</given-names></name>
<name><surname>Ogusuku</surname><given-names>L.M.</given-names></name>
<name><surname>Keese Albertini</surname><given-names>M.</given-names></name>
<name><surname>Pimentel</surname><given-names>R.</given-names></name>
<name><surname>Backes</surname><given-names>A.R.</given-names></name>
</person-group><article-title>Wild boar recognition using convolutional neural networks</article-title><source>Concurr. Comput. Pract. Exp.</source><year>2021</year><volume>33</volume><fpage>e6010</fpage><pub-id pub-id-type="doi">10.1002/cpe.6010</pub-id></element-citation></ref><ref id="B8-animals-13-00838"><label>8.</label><element-citation publication-type="confproc"><person-group person-group-type="author">
<name><surname>Nguyen</surname><given-names>H.</given-names></name>
<name><surname>Maclagan</surname><given-names>S.J.</given-names></name>
<name><surname>Nguyen</surname><given-names>T.D.</given-names></name>
<name><surname>Nguyen</surname><given-names>T.</given-names></name>
<name><surname>Flemons</surname><given-names>P.</given-names></name>
<name><surname>Andrews</surname><given-names>K.</given-names></name>
<name><surname>Ritchie</surname><given-names>E.G.</given-names></name>
<name><surname>Phung</surname><given-names>D.</given-names></name>
</person-group><article-title>Animal recognition and identification with deep convolutional neural networks for automated wildlife monitoring</article-title><source>Proceedings of the 2017 IEEE International Conference on Data Science and Advanced Analytics (DSAA)</source><conf-loc>Tokyo, Japan</conf-loc><conf-date>19&#x02013;21 October 2017</conf-date><publisher-name>IEEE</publisher-name><publisher-loc>New York, NY, USA</publisher-loc><year>2017</year><fpage>40</fpage><lpage>49</lpage></element-citation></ref><ref id="B9-animals-13-00838"><label>9.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Geirhos</surname><given-names>R.</given-names></name>
<name><surname>Jacobsen</surname><given-names>J.H.</given-names></name>
<name><surname>Michaelis</surname><given-names>C.</given-names></name>
<name><surname>Zemel</surname><given-names>R.</given-names></name>
<name><surname>Brendel</surname><given-names>W.</given-names></name>
<name><surname>Bethge</surname><given-names>M.</given-names></name>
<name><surname>Wichmann</surname><given-names>F.A.</given-names></name>
</person-group><article-title>Shortcut learning in deep neural networks</article-title><source>Nat. Mach. Intell.</source><year>2020</year><volume>2</volume><fpage>665</fpage><lpage>673</lpage><pub-id pub-id-type="doi">10.1038/s42256-020-00257-z</pub-id></element-citation></ref><ref id="B10-animals-13-00838"><label>10.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Szegedy</surname><given-names>C.</given-names></name>
<name><surname>Zaremba</surname><given-names>W.</given-names></name>
<name><surname>Sutskever</surname><given-names>I.</given-names></name>
<name><surname>Bruna</surname><given-names>J.</given-names></name>
<name><surname>Erhan</surname><given-names>D.</given-names></name>
<name><surname>Goodfellow</surname><given-names>I.</given-names></name>
<name><surname>Fergus</surname><given-names>R.</given-names></name>
</person-group><article-title>Intriguing properties of neural networks</article-title><source>arXiv</source><year>2013</year><pub-id pub-id-type="arxiv">1312.6199</pub-id></element-citation></ref><ref id="B11-animals-13-00838"><label>11.</label><element-citation publication-type="confproc"><person-group person-group-type="author">
<name><surname>Cubuk</surname><given-names>E.D.</given-names></name>
<name><surname>Zoph</surname><given-names>B.</given-names></name>
<name><surname>Mane</surname><given-names>D.</given-names></name>
<name><surname>Vasudevan</surname><given-names>V.</given-names></name>
<name><surname>Le</surname><given-names>Q.V.</given-names></name>
</person-group><article-title>Autoaugment: Learning augmentation strategies from data</article-title><source>Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</source><conf-loc>Long Beach, CA, USA</conf-loc><conf-date>15&#x02013;20 June 2019</conf-date><fpage>113</fpage><lpage>123</lpage></element-citation></ref><ref id="B12-animals-13-00838"><label>12.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Arjovsky</surname><given-names>M.</given-names></name>
<name><surname>Bottou</surname><given-names>L.</given-names></name>
<name><surname>Gulrajani</surname><given-names>I.</given-names></name>
<name><surname>Lopez-Paz</surname><given-names>D.</given-names></name>
</person-group><article-title>Invariant risk minimization</article-title><source>arXiv</source><year>2019</year><pub-id pub-id-type="arxiv">1907.02893</pub-id></element-citation></ref><ref id="B13-animals-13-00838"><label>13.</label><element-citation publication-type="confproc"><person-group person-group-type="author">
<name><surname>Finn</surname><given-names>C.</given-names></name>
<name><surname>Abbeel</surname><given-names>P.</given-names></name>
<name><surname>Levine</surname><given-names>S.</given-names></name>
</person-group><article-title>Model-agnostic meta-learning for fast adaptation of deep networks</article-title><source>Proceedings of the International Conference on Machine Learning</source><conf-loc>Sydney, Australia</conf-loc><conf-date>6&#x02013;11 August 2017</conf-date><fpage>1126</fpage><lpage>1135</lpage></element-citation></ref><ref id="B14-animals-13-00838"><label>14.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Jia</surname><given-names>L.</given-names></name>
<name><surname>Tian</surname><given-names>Y.</given-names></name>
<name><surname>Zhang</surname><given-names>J.</given-names></name>
</person-group><article-title>Domain-Aware Neural Architecture Search for Classifying Animals in Camera Trap Images</article-title><source>Animals</source><year>2022</year><volume>12</volume><elocation-id>437</elocation-id><pub-id pub-id-type="doi">10.3390/ani12040437</pub-id><pub-id pub-id-type="pmid">35203145</pub-id></element-citation></ref><ref id="B15-animals-13-00838"><label>15.</label><element-citation publication-type="confproc"><person-group person-group-type="author">
<name><surname>Bucilu&#x001ce;</surname><given-names>C.</given-names></name>
<name><surname>Caruana</surname><given-names>R.</given-names></name>
<name><surname>Niculescu-Mizil</surname><given-names>A.</given-names></name>
</person-group><article-title>Model compression</article-title><source>Proceedings of the 12th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining</source><conf-loc>Philadelphia, PA, USA</conf-loc><conf-date>20&#x02013;23 August 2006</conf-date><fpage>535</fpage><lpage>541</lpage></element-citation></ref><ref id="B16-animals-13-00838"><label>16.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Hinton</surname><given-names>G.</given-names></name>
<name><surname>Vinyals</surname><given-names>O.</given-names></name>
<name><surname>Dean</surname><given-names>J.</given-names></name>
</person-group><article-title>Distilling the knowledge in a neural network</article-title><source>arXiv</source><year>2015</year><comment>
<italic toggle="yes">2</italic>
</comment><pub-id pub-id-type="arxiv">1503.02531</pub-id></element-citation></ref><ref id="B17-animals-13-00838"><label>17.</label><element-citation publication-type="confproc"><person-group person-group-type="author">
<name><surname>Wen</surname><given-names>W.</given-names></name>
<name><surname>Wu</surname><given-names>C.</given-names></name>
<name><surname>Wang</surname><given-names>Y.</given-names></name>
<name><surname>Chen</surname><given-names>Y.</given-names></name>
<name><surname>Li</surname><given-names>H.</given-names></name>
</person-group><article-title>Learning structured sparsity in deep neural networks</article-title><source>Proceedings of the Advances in Neural Information Processing Systems 29 (NIPS 2016): Annual Conference on Neural Information Processing Systems</source><conf-loc>Barcelona, Spain</conf-loc><conf-date>5&#x02013;10 December 2016</conf-date><volume>Volume 29</volume></element-citation></ref><ref id="B18-animals-13-00838"><label>18.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Heo</surname><given-names>D.</given-names></name>
<name><surname>Nam</surname><given-names>J.Y.</given-names></name>
<name><surname>Ko</surname><given-names>B.C.</given-names></name>
</person-group><article-title>Estimation of pedestrian pose orientation using soft target training based on teacher&#x02013;student framework</article-title><source>Sensors</source><year>2019</year><volume>19</volume><elocation-id>1147</elocation-id><pub-id pub-id-type="doi">10.3390/s19051147</pub-id><pub-id pub-id-type="pmid">30845772</pub-id></element-citation></ref><ref id="B19-animals-13-00838"><label>19.</label><element-citation publication-type="confproc"><person-group person-group-type="author">
<name><surname>Crowley</surname><given-names>E.J.</given-names></name>
<name><surname>Gray</surname><given-names>G.</given-names></name>
<name><surname>Storkey</surname><given-names>A.J.</given-names></name>
</person-group><article-title>Moonshine: Distilling with cheap convolutions</article-title><source>Proceedings of the Advances in Neural Information Processing Systems 31: Annual Conference on Neural Information Processing Systems</source><conf-loc>Montreal, QC, Canada</conf-loc><conf-date>3&#x02013;8 December 2018; Volme 31</conf-date></element-citation></ref><ref id="B20-animals-13-00838"><label>20.</label><element-citation publication-type="confproc"><person-group person-group-type="author">
<name><surname>Chen</surname><given-names>W.</given-names></name>
<name><surname>Wilson</surname><given-names>J.</given-names></name>
<name><surname>Tyree</surname><given-names>S.</given-names></name>
<name><surname>Weinberger</surname><given-names>K.</given-names></name>
<name><surname>Chen</surname><given-names>Y.</given-names></name>
</person-group><article-title>Compressing neural networks with the hashing trick</article-title><source>Proceedings of the International Conference on Machine Learning</source><conf-loc>Lille, France</conf-loc><conf-date>6&#x02013;11 July 2015</conf-date><fpage>2285</fpage><lpage>2294</lpage></element-citation></ref><ref id="B21-animals-13-00838"><label>21.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Li</surname><given-names>H.</given-names></name>
<name><surname>Kadav</surname><given-names>A.</given-names></name>
<name><surname>Durdanovic</surname><given-names>I.</given-names></name>
<name><surname>Samet</surname><given-names>H.</given-names></name>
<name><surname>Graf</surname><given-names>H.P.</given-names></name>
</person-group><article-title>Pruning filters for efficient convnets</article-title><source>arXiv</source><year>2016</year><pub-id pub-id-type="arxiv">1608.08710</pub-id></element-citation></ref><ref id="B22-animals-13-00838"><label>22.</label><element-citation publication-type="confproc"><person-group person-group-type="author">
<name><surname>Luo</surname><given-names>J.H.</given-names></name>
<name><surname>Wu</surname><given-names>J.</given-names></name>
<name><surname>Lin</surname><given-names>W.</given-names></name>
</person-group><article-title>Thinet: A filter level pruning method for deep neural network compression</article-title><source>Proceedings of the IEEE International Conference on Computer Vision</source><conf-loc>Venice, Italy</conf-loc><conf-date>22&#x02013;29 October 2017</conf-date><fpage>5058</fpage><lpage>5066</lpage></element-citation></ref><ref id="B23-animals-13-00838"><label>23.</label><element-citation publication-type="confproc"><person-group person-group-type="author">
<name><surname>He</surname><given-names>Y.</given-names></name>
<name><surname>Zhang</surname><given-names>X.</given-names></name>
<name><surname>Sun</surname><given-names>J.</given-names></name>
</person-group><article-title>Channel pruning for accelerating very deep neural networks</article-title><source>Proceedings of the IEEE International Conference on Computer Vision</source><conf-loc>Venice, Italy</conf-loc><conf-date>22&#x02013;29 October 2017</conf-date><fpage>1389</fpage><lpage>1397</lpage></element-citation></ref><ref id="B24-animals-13-00838"><label>24.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Jin</surname><given-names>L.L.</given-names></name>
<name><surname>Yang</surname><given-names>W.Z.</given-names></name>
<name><surname>Wang</surname><given-names>S.L.</given-names></name>
</person-group><article-title>Mixed pruning method for convolutional neural network compression</article-title><source>J. Chin. Comput. Syst.</source><year>2018</year><volume>39</volume><fpage>2596</fpage><lpage>2601</lpage></element-citation></ref><ref id="B25-animals-13-00838"><label>25.</label><element-citation publication-type="confproc"><person-group person-group-type="author">
<name><surname>Aghli</surname><given-names>N.</given-names></name>
<name><surname>Ribeiro</surname><given-names>E.</given-names></name>
</person-group><article-title>Combining weight pruning and knowledge distillation for cnn compression</article-title><source>Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</source><conf-loc>Nashville, TN, USA</conf-loc><conf-date>20&#x02013;25 June 2021</conf-date><fpage>3191</fpage><lpage>3198</lpage></element-citation></ref><ref id="B26-animals-13-00838"><label>26.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Chen</surname><given-names>S.</given-names></name>
<name><surname>Hu</surname><given-names>C.</given-names></name>
<name><surname>Zhang</surname><given-names>J.</given-names></name>
</person-group><article-title>Design of wildlife image monitoring system based on wireless sensor networks</article-title><source>Mod. Manuf. Technol. Equip.</source><year>2017</year><volume>3</volume><fpage>64</fpage><lpage>66</lpage></element-citation></ref><ref id="B27-animals-13-00838"><label>27.</label><element-citation publication-type="book"><person-group person-group-type="author">
<name><surname>Na</surname><given-names>L.</given-names></name>
</person-group><article-title>Nature Monitoring on Wildlife Biodiversity at Saihanwula National Nature Reserve</article-title><source>Ph.D. Thesis</source><publisher-name>Beijing Forestry University</publisher-name><publisher-loc>Beijing, China</publisher-loc><year>2011</year></element-citation></ref><ref id="B28-animals-13-00838"><label>28.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Russell</surname><given-names>B.C.</given-names></name>
<name><surname>Torralba</surname><given-names>A.</given-names></name>
<name><surname>Murphy</surname><given-names>K.P.</given-names></name>
<name><surname>Freeman</surname><given-names>W.T.</given-names></name>
</person-group><article-title>LabelMe: A database and web-based tool for image annotation</article-title><source>Int. J. Comput. Vis.</source><year>2008</year><volume>77</volume><fpage>157</fpage><lpage>173</lpage><pub-id pub-id-type="doi">10.1007/s11263-007-0090-8</pub-id></element-citation></ref><ref id="B29-animals-13-00838"><label>29.</label><element-citation publication-type="confproc"><person-group person-group-type="author">
<name><surname>Ahn</surname><given-names>J.</given-names></name>
<name><surname>Cho</surname><given-names>S.</given-names></name>
<name><surname>Kwak</surname><given-names>S.</given-names></name>
</person-group><article-title>Weakly supervised learning of instance segmentation with inter-pixel relations</article-title><source>Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</source><conf-loc>Long Beach, CA, USA</conf-loc><conf-date>15&#x02013;20 June 201</conf-date><fpage>2209</fpage><lpage>2218</lpage></element-citation></ref><ref id="B30-animals-13-00838"><label>30.</label><element-citation publication-type="confproc"><person-group person-group-type="author">
<name><surname>Li</surname><given-names>B.</given-names></name>
<name><surname>Wu</surname><given-names>B.</given-names></name>
<name><surname>Su</surname><given-names>J.</given-names></name>
<name><surname>Wang</surname><given-names>G.</given-names></name>
</person-group><article-title>Eagleeye: Fast sub-net evaluation for efficient neural network pruning</article-title><source>Proceedings of the European Conference on Computer Vision</source><conf-loc>Glasgow, UK</conf-loc><conf-date>23&#x02013;28 August 2020</conf-date><publisher-name>Springer</publisher-name><publisher-loc>Berlin/Heidelberg, Germany</publisher-loc><year>2020</year><fpage>639</fpage><lpage>654</lpage></element-citation></ref><ref id="B31-animals-13-00838"><label>31.</label><element-citation publication-type="confproc"><person-group person-group-type="author">
<name><surname>Ioffe</surname><given-names>S.</given-names></name>
<name><surname>Szegedy</surname><given-names>C.</given-names></name>
</person-group><article-title>Batch normalization: Accelerating deep network training by reducing internal covariate shift</article-title><source>Proceedings of the International Conference on Machine Learning</source><conf-loc>Lille, France</conf-loc><conf-date>6&#x02013;11 July 2015</conf-date><fpage>448</fpage><lpage>456</lpage></element-citation></ref><ref id="B32-animals-13-00838"><label>32.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Bauer</surname><given-names>E.</given-names></name>
<name><surname>Kohavi</surname><given-names>R.</given-names></name>
</person-group><article-title>An empirical comparison of voting classification algorithms: Bagging, boosting, and variants</article-title><source>Mach. Learn.</source><year>1999</year><volume>36</volume><fpage>105</fpage><lpage>139</lpage><pub-id pub-id-type="doi">10.1023/A:1007515423169</pub-id></element-citation></ref><ref id="B33-animals-13-00838"><label>33.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Nakandala</surname><given-names>S.</given-names></name>
<name><surname>Nagrecha</surname><given-names>K.</given-names></name>
<name><surname>Kumar</surname><given-names>A.</given-names></name>
<name><surname>Papakonstantinou</surname><given-names>Y.</given-names></name>
</person-group><article-title>Incremental and approximate computations for accelerating deep CNN inference</article-title><source>ACM Trans. Database Syst. (TODS)</source><year>2020</year><volume>45</volume><fpage>1</fpage><lpage>42</lpage><pub-id pub-id-type="doi">10.1145/3397461</pub-id></element-citation></ref><ref id="B34-animals-13-00838"><label>34.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>DeVries</surname><given-names>T.</given-names></name>
<name><surname>Taylor</surname><given-names>G.W.</given-names></name>
</person-group><article-title>Improved regularization of convolutional neural networks with cutout</article-title><source>arXiv</source><year>2017</year><pub-id pub-id-type="arxiv">1708.04552</pub-id></element-citation></ref><ref id="B35-animals-13-00838"><label>35.</label><element-citation publication-type="confproc"><person-group person-group-type="author">
<name><surname>Selvaraju</surname><given-names>R.R.</given-names></name>
<name><surname>Cogswell</surname><given-names>M.</given-names></name>
<name><surname>Das</surname><given-names>A.</given-names></name>
<name><surname>Vedantam</surname><given-names>R.</given-names></name>
<name><surname>Parikh</surname><given-names>D.</given-names></name>
<name><surname>Batra</surname><given-names>D.</given-names></name>
</person-group><article-title>Grad-cam: Visual explanations from deep networks via gradient-based localization</article-title><source>Proceedings of the IEEE International Conference on Computer Vision</source><conf-loc>Venice, Italy</conf-loc><conf-date>22&#x02013;29 October 2017</conf-date><fpage>618</fpage><lpage>626</lpage></element-citation></ref><ref id="B36-animals-13-00838"><label>36.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Zualkernan</surname><given-names>I.</given-names></name>
<name><surname>Dhou</surname><given-names>S.</given-names></name>
<name><surname>Judas</surname><given-names>J.</given-names></name>
<name><surname>Sajun</surname><given-names>A.R.</given-names></name>
<name><surname>Gomez</surname><given-names>B.R.</given-names></name>
<name><surname>Hussain</surname><given-names>L.A.</given-names></name>
</person-group><article-title>An IoT System Using Deep Learning to Classify Camera Trap Images on the Edge</article-title><source>Computers</source><year>2022</year><volume>11</volume><elocation-id>13</elocation-id><pub-id pub-id-type="doi">10.3390/computers11010013</pub-id></element-citation></ref><ref id="B37-animals-13-00838"><label>37.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Wu</surname><given-names>T.Y.</given-names></name>
<name><surname>Yeh</surname><given-names>K.T.</given-names></name>
<name><surname>Hsu</surname><given-names>H.C.</given-names></name>
<name><surname>Yang</surname><given-names>C.K.</given-names></name>
<name><surname>Tsai</surname><given-names>M.J.</given-names></name>
<name><surname>Kuo</surname><given-names>Y.F.</given-names></name>
</person-group><article-title>Identifying Fagaceae and Lauraceae species using leaf images and convolutional neural networks</article-title><source>Ecol. Inform.</source><year>2022</year><volume>68</volume><fpage>101513</fpage><pub-id pub-id-type="doi">10.1016/j.ecoinf.2021.101513</pub-id></element-citation></ref><ref id="B38-animals-13-00838"><label>38.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Xie</surname><given-names>J.</given-names></name>
<name><surname>Zhao</surname><given-names>S.</given-names></name>
<name><surname>Li</surname><given-names>X.</given-names></name>
<name><surname>Ni</surname><given-names>D.</given-names></name>
<name><surname>Zhang</surname><given-names>J.</given-names></name>
</person-group><article-title>KD-CLDNN: Lightweight automatic recognition model based on bird vocalization</article-title><source>Appl. Acoust.</source><year>2022</year><volume>188</volume><fpage>108550</fpage><pub-id pub-id-type="doi">10.1016/j.apacoust.2021.108550</pub-id></element-citation></ref><ref id="B39-animals-13-00838"><label>39.</label><element-citation publication-type="book"><person-group person-group-type="author">
<name><surname>Tyd&#x000e9;n</surname><given-names>A.</given-names></name>
<name><surname>Olsson</surname><given-names>S.</given-names></name>
</person-group><article-title>Edge Machine Learning for Animal Detection, Classification, and Tracking</article-title><source>Ph.D. Thesis</source><publisher-name>Linkoping University</publisher-name><publisher-loc>Linkoping, Sweden</publisher-loc><year>2020</year></element-citation></ref><ref id="B40-animals-13-00838"><label>40.</label><element-citation publication-type="journal"><person-group person-group-type="author">
<name><surname>Kim</surname><given-names>T.</given-names></name>
<name><surname>Oh</surname><given-names>J.</given-names></name>
<name><surname>Kim</surname><given-names>N.</given-names></name>
<name><surname>Cho</surname><given-names>S.</given-names></name>
<name><surname>Yun</surname><given-names>S.Y.</given-names></name>
</person-group><article-title>Comparing kullback-leibler divergence and mean squared error loss in knowledge distillation</article-title><source>arXiv</source><year>2021</year><pub-id pub-id-type="arxiv">2105.08919</pub-id></element-citation></ref></ref-list></back><floats-group><fig position="float" id="animals-13-00838-f001"><label>Figure 1</label><caption><p>The technological framework for the design of the lightweight automatic wildlife recognition model.</p></caption><graphic xlink:href="animals-13-00838-g001" position="float"/></fig><fig position="float" id="animals-13-00838-f002"><label>Figure 2</label><caption><p>Image synthesis method.</p></caption><graphic xlink:href="animals-13-00838-g002" position="float"/></fig><fig position="float" id="animals-13-00838-f003"><label>Figure 3</label><caption><p>Synthetic samples.</p></caption><graphic xlink:href="animals-13-00838-g003" position="float"/></fig><fig position="float" id="animals-13-00838-f004"><label>Figure 4</label><caption><p>Fine-tuning based on knowledge distillation. (<bold>a</bold>) indicates mean square error (MSE) distillation loss and (<bold>b</bold>) indicates hard soft kullback-leibler (KL) distillation loss.</p></caption><graphic xlink:href="animals-13-00838-g004" position="float"/></fig><fig position="float" id="animals-13-00838-f005"><label>Figure 5</label><caption><p>Schematic diagram of FRoH.</p></caption><graphic xlink:href="animals-13-00838-g005" position="float"/></fig><fig position="float" id="animals-13-00838-f006"><label>Figure 6</label><caption><p>Heatmaps of typical images. The first row contains the input images, the second row contains the heatmaps of images without the mixed augmentation method, and the third row contains the heatmaps of images with mixed augmentation method.</p></caption><graphic xlink:href="animals-13-00838-g006" position="float"/></fig><fig position="float" id="animals-13-00838-f007"><label>Figure 7</label><caption><p>Recognition results with or without mixed augmentation method. From up to down, there are the images of goral (label 0), badger (label 1), red deer (label 2), roe deer (label 3). The first column is the input images, second column are the heatmaps of images without mixed augmentation method, the third column is the heatmaps of images with mixed augmentation method. The top left corner of each heatmap presents the predicted label.</p></caption><graphic xlink:href="animals-13-00838-g007" position="float"/></fig><fig position="float" id="animals-13-00838-f008"><label>Figure 8</label><caption><p>Confusion matrix. (<bold>a</bold>) indicates class wise accuracies of ResNet-50 and (<bold>b</bold>) indicates class wise accuracies of our method (+Mixed(+RBS+IS)).</p></caption><graphic xlink:href="animals-13-00838-g008" position="float"/></fig><fig position="float" id="animals-13-00838-f009"><label>Figure 9</label><caption><p>Fitness variations of the sub-networks during the search under 50 &#x000b1; 5% compression ratio. The solid line shows the sliding average of the fitness (<italic toggle="yes">N</italic> = 3).</p></caption><graphic xlink:href="animals-13-00838-g009" position="float"/></fig><table-wrap position="float" id="animals-13-00838-t001"><object-id pub-id-type="pii">animals-13-00838-t001_Table 1</object-id><label>Table 1</label><caption><p>Details of Wildlife-6.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">Species</th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">Number of Images</th></tr></thead><tbody><tr><td align="center" valign="middle" rowspan="1" colspan="1">Red deer</td><td align="center" valign="middle" rowspan="1" colspan="1">1094</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">Goral</td><td align="center" valign="middle" rowspan="1" colspan="1">761</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">Roe deer</td><td align="center" valign="middle" rowspan="1" colspan="1">1310</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">Lynx</td><td align="center" valign="middle" rowspan="1" colspan="1">377</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">Badger</td><td align="center" valign="middle" rowspan="1" colspan="1">190</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Wild boar</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">906</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Total</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">4638</td></tr></tbody></table></table-wrap><table-wrap position="float" id="animals-13-00838-t002"><object-id pub-id-type="pii">animals-13-00838-t002_Table 2</object-id><label>Table 2</label><caption><p>Setup of the experiments.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">Test Environment</th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">Type</th></tr></thead><tbody><tr><td align="center" valign="middle" rowspan="1" colspan="1">System</td><td align="center" valign="middle" rowspan="1" colspan="1">Ubuntu16.04</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">Graphics card</td><td align="center" valign="middle" rowspan="1" colspan="1">Nvidia 1080 ti (11 GB/Nvidia)</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">CPU</td><td align="center" valign="middle" rowspan="1" colspan="1">Intel Core i5-9400F @ 2.9GHz</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">RAM</td><td align="center" valign="middle" rowspan="1" colspan="1">32 GB</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">Framework</td><td align="center" valign="middle" rowspan="1" colspan="1">Pytorch1.4.1</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Programming language</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Python3.6</td></tr></tbody></table></table-wrap><table-wrap position="float" id="animals-13-00838-t003"><object-id pub-id-type="pii">animals-13-00838-t003_Table 3</object-id><label>Table 3</label><caption><p>Training settings.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">Item</th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">Value or Method</th></tr></thead><tbody><tr><td align="center" valign="middle" rowspan="1" colspan="1">Optimizer</td><td align="center" valign="middle" rowspan="1" colspan="1">SGD</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">Momentum</td><td align="center" valign="middle" rowspan="1" colspan="1">0.9</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">Initial learning rate</td><td align="center" valign="middle" rowspan="1" colspan="1">0.0001</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">Learning rate decay</td><td align="center" valign="middle" rowspan="1" colspan="1">cosine decay</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">Batch size</td><td align="center" valign="middle" rowspan="1" colspan="1">64</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">Iterative scheme</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">early stop, patience = 50</td></tr></tbody></table><table-wrap-foot><fn><p>SGD = stochastic gradient descent.</p></fn></table-wrap-foot></table-wrap><table-wrap position="float" id="animals-13-00838-t004"><object-id pub-id-type="pii">animals-13-00838-t004_Table 4</object-id><label>Table 4</label><caption><p>Results of different data augmentation methods.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="center" valign="middle" style="border-top:solid thin; border-bottom:solid thin" rowspan="1" colspan="1">Methods</th><th align="center" valign="middle" style="border-top:solid thin; border-bottom:solid thin" rowspan="1" colspan="1">
<inline-formula>
<mml:math id="mm30" overflow="scroll"><mml:mrow><mml:mstyle mathvariant="bold"><mml:msub><mml:mi>Acc</mml:mi><mml:mrow><mml:mi>cls</mml:mi><mml:mspace width="4.pt"/></mml:mrow></mml:msub></mml:mstyle></mml:mrow></mml:math>
</inline-formula>
</th><th align="center" valign="middle" style="border-top:solid thin; border-bottom:solid thin" rowspan="1" colspan="1">
<inline-formula>
<mml:math id="mm31" overflow="scroll"><mml:mrow><mml:mstyle mathvariant="bold"><mml:mi>FRoH</mml:mi></mml:mstyle></mml:mrow></mml:math>
</inline-formula>
</th></tr></thead><tbody><tr><td align="center" valign="middle" rowspan="1" colspan="1">ResNet-50</td><td align="center" valign="middle" rowspan="1" colspan="1">89.76%</td><td align="center" valign="middle" rowspan="1" colspan="1">43.40%</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">+ Repeat sampling</td><td align="center" valign="middle" rowspan="1" colspan="1">88.23%</td><td align="center" valign="middle" rowspan="1" colspan="1">43.36%</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">+ Cutout</td><td align="center" valign="middle" rowspan="1" colspan="1">89.03%</td><td align="center" valign="middle" rowspan="1" colspan="1">43.83%</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">+RBS</td><td align="center" valign="middle" rowspan="1" colspan="1">90.13%</td><td align="center" valign="middle" rowspan="1" colspan="1">43.57%</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">+ IS</td><td align="center" valign="middle" rowspan="1" colspan="1">90.72%</td><td align="center" valign="middle" rowspan="1" colspan="1">43.95%</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">+Mixed (+RBS + IS)</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">91.23%</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">44.71%</td></tr></tbody></table><table-wrap-foot><fn><p>RBS = regional background suppression; IS = image synthesis.</p></fn></table-wrap-foot></table-wrap><table-wrap position="float" id="animals-13-00838-t005"><object-id pub-id-type="pii">animals-13-00838-t005_Table 5</object-id><label>Table 5</label><caption><p>Results of the NACTI dataset.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="center" valign="middle" style="border-top:solid thin; border-bottom:solid thin" rowspan="1" colspan="1">Methods</th><th align="center" valign="middle" style="border-top:solid thin; border-bottom:solid thin" rowspan="1" colspan="1">
<inline-formula>
<mml:math id="mm32" overflow="scroll"><mml:mrow><mml:mstyle mathvariant="bold"><mml:msub><mml:mi>Acc</mml:mi><mml:mrow><mml:mi>cls</mml:mi><mml:mspace width="4.pt"/></mml:mrow></mml:msub></mml:mstyle></mml:mrow></mml:math>
</inline-formula>
</th><th align="center" valign="middle" style="border-top:solid thin; border-bottom:solid thin" rowspan="1" colspan="1">
<inline-formula>
<mml:math id="mm33" overflow="scroll"><mml:mrow><mml:mstyle mathvariant="bold"><mml:mi>FRoH</mml:mi></mml:mstyle></mml:mrow></mml:math>
</inline-formula>
</th></tr></thead><tbody><tr><td align="center" valign="middle" rowspan="1" colspan="1">ResNet-50</td><td align="center" valign="middle" rowspan="1" colspan="1">99.21%</td><td align="center" valign="middle" rowspan="1" colspan="1">46.48%</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">+Mixed (+RBS + IS)</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">99.24%</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">49.35%</td></tr></tbody></table><table-wrap-foot><fn><p>RBS = regional background suppression; IS = image synthesis.</p></fn></table-wrap-foot></table-wrap><table-wrap position="float" id="animals-13-00838-t006"><object-id pub-id-type="pii">animals-13-00838-t006_Table 6</object-id><label>Table 6</label><caption><p>Performances of ResNet50 and the pruned models with compression ratio of 50 &#x000b1; 5%.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">
</th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">ResNet50</th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">Random Sampling</th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">Our Method</th></tr></thead><tbody><tr><td align="center" valign="middle" rowspan="1" colspan="1">Accuracy</td><td align="center" valign="middle" rowspan="1" colspan="1">91.23%</td><td align="center" valign="middle" rowspan="1" colspan="1">84.90%</td><td align="center" valign="middle" rowspan="1" colspan="1">86.50%</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">Parameters</td><td align="center" valign="middle" rowspan="1" colspan="1">23.52 M</td><td align="center" valign="middle" rowspan="1" colspan="1">10.65 M</td><td align="center" valign="middle" rowspan="1" colspan="1">10.01 M</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">FLOPs</td><td align="center" valign="middle" rowspan="1" colspan="1">16.48 G</td><td align="center" valign="middle" rowspan="1" colspan="1">8.85 G</td><td align="center" valign="middle" rowspan="1" colspan="1">8.89 G</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">FPS</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">0.85</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">53.2</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">53.49</td></tr></tbody></table><table-wrap-foot><fn><p>FLOPs = floating-point operations per second; FPS = frames per second.</p></fn></table-wrap-foot></table-wrap><table-wrap position="float" id="animals-13-00838-t007"><object-id pub-id-type="pii">animals-13-00838-t007_Table 7</object-id><label>Table 7</label><caption><p>Performances of the pruned models with compression ratio of 25 &#x000b1; 5%.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">
</th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">Random Sampling</th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">Our Method</th></tr></thead><tbody><tr><td align="center" valign="middle" rowspan="1" colspan="1">Accuracy</td><td align="center" valign="middle" rowspan="1" colspan="1">83.80%</td><td align="center" valign="middle" rowspan="1" colspan="1">86.20%</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">Parameters</td><td align="center" valign="middle" rowspan="1" colspan="1">5.65 M</td><td align="center" valign="middle" rowspan="1" colspan="1">5.65 M</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">FLOPs</td><td align="center" valign="middle" rowspan="1" colspan="1">4.73 G</td><td align="center" valign="middle" rowspan="1" colspan="1">4.73 G</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">FPS</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">56.62</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">56.62</td></tr></tbody></table><table-wrap-foot><fn><p>FLOPs = floating-point operations per second; FPS = frames per second.</p></fn></table-wrap-foot></table-wrap><table-wrap position="float" id="animals-13-00838-t008"><object-id pub-id-type="pii">animals-13-00838-t008_Table 8</object-id><label>Table 8</label><caption><p>Result of different fine-tuning framework on our dataset.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">Method</th><th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" rowspan="1" colspan="1">Accuracy</th></tr></thead><tbody><tr><td align="center" valign="middle" rowspan="1" colspan="1">P-ResNet50</td><td align="center" valign="middle" rowspan="1" colspan="1">86.50%</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">KD-SKL</td><td align="center" valign="middle" rowspan="1" colspan="1">87.72%</td></tr><tr><td align="center" valign="middle" rowspan="1" colspan="1">KD-HKL</td><td align="center" valign="middle" rowspan="1" colspan="1">86.99%</td></tr><tr><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">KD-MSE</td><td align="center" valign="middle" style="border-bottom:solid thin" rowspan="1" colspan="1">88.38%</td></tr></tbody></table><table-wrap-foot><fn><p>KD-SKL = knowledge distillation with soft kullback-leibler; KD-HKL = knowledge distillation with hard kullbackleibler; KD-MSE = knowledge distillation with mean square error.</p></fn></table-wrap-foot></table-wrap></floats-group></article>
