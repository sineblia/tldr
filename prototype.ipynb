{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "importing libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import library to extract data from XML file\n",
    "import xml.etree.ElementTree as ET\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "import xml.etree.ElementTree as ET"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "extract_data function definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\"\"\"\n",
    "    Extracts data from an XML file and returns it as a dictionary.\n",
    "\n",
    "    Args:\n",
    "        file (str): The path to the XML file.\n",
    "\n",
    "    Returns:\n",
    "        dict: A dictionary containing the extracted data.\n",
    "\"\"\"\n",
    "def extract_data(file):\n",
    "    # Create a dictionary to store the data\n",
    "    data = {}\n",
    "    # Parse the XML file\n",
    "    tree = ET.parse(file)\n",
    "    # Get the root of the XML file\n",
    "    root = tree.getroot()\n",
    "\n",
    "    # Initialize abstract data\n",
    "    data['abstract'] = {}\n",
    "\n",
    "    # Initialize body data\n",
    "    data['body'] = []\n",
    "\n",
    "    # Extract title and abstract\n",
    "    article_meta = root.find('.//article-meta')\n",
    "    if article_meta is not None:\n",
    "        title_group = article_meta.find('title-group')\n",
    "        data['title'] = title_group.find('article-title').text if title_group is not None else None\n",
    "\n",
    "        abstract_section = article_meta.find('abstract')\n",
    "        if abstract_section is not None:\n",
    "            for section in abstract_section.findall('sec'):\n",
    "                section_title = section.find('title').text if section.find('title') is not None else ''\n",
    "                section_text = section.find('p').text if section.find('p') is not None else ''\n",
    "                if 'simple summary' in section_title.lower():\n",
    "                    data['abstract']['simple_summary'] = section_text\n",
    "                elif 'abstract' in section_title.lower():\n",
    "                    data['abstract']['abstract'] = section_text\n",
    "\n",
    "    # Extract body sections\n",
    "    body_section = root.find('body')\n",
    "    if body_section is not None:\n",
    "        for sec in body_section.findall('sec'):\n",
    "            section_data = {\n",
    "                'title': sec.find('title').text if sec.find('title') is not None else None,\n",
    "                'content': [p.text for p in sec.findall('p') if p.text]\n",
    "            }\n",
    "            data['body'].append(section_data)\n",
    "\n",
    "    # Extract references\n",
    "    references_section = root.find('.//ref-list')\n",
    "    if references_section is not None:\n",
    "        data['references'] = [ET.tostring(reference, encoding='unicode') for reference in references_section.findall('ref')]\n",
    "\n",
    "    # Return the extracted data\n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "creating the dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract data from the XML files\n",
    "# Create a list to store the data\n",
    "data = []\n",
    "# Get the path of the XML files\n",
    "path = './data'\n",
    "\n",
    "# Get the list of the XML files\n",
    "files = os.listdir(path)\n",
    "\n",
    "# Loop through the XML files\n",
    "for file in files:\n",
    "    # Extract data from the XML file\n",
    "    data.append(extract_data(path + '/' + file))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the list of dictionaries to a Pandas DataFrame\n",
    "df = pd.DataFrame(data)\n",
    "# Save the DataFrame as a JSON file\n",
    "df.to_json('data.json', orient='records')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "importing libraries to preprocess data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import spacy\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# Download necessary NLTK data\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# Load English tokenizer, tagger, parser, NER and word vectors\n",
    "nlp = spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "defining preprocessing functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(text):\n",
    "    \"\"\"Divide the text into tokens (words).\"\"\"\n",
    "    return word_tokenize(text)\n",
    "\n",
    "def normalize(text):\n",
    "    \"\"\"Convert text to lowercase and remove punctuation.\"\"\"\n",
    "    text = text.lower()\n",
    "    # Remove punctuation characters\n",
    "    text = ''.join([char for char in text if char.isalpha() or char.isspace()])\n",
    "    return text\n",
    "\n",
    "def remove_stopwords(tokens):\n",
    "    \"\"\"Remove stopwords from the list of tokens.\"\"\"\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    return [word for word in tokens if not word in stop_words]\n",
    "\n",
    "def stem(tokens):\n",
    "    \"\"\"Apply stemming to the tokens.\"\"\"\n",
    "    stemmer = PorterStemmer()\n",
    "    return [stemmer.stem(word) for word in tokens]\n",
    "\n",
    "def lemmatize(text):\n",
    "    \"\"\"Apply lemmatization to the text.\"\"\"\n",
    "    doc = nlp(text)\n",
    "    return [token.lemma_ for token in doc]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "actual preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# iterate over data list to print its contents\n",
    "for i in range(len(data)):\n",
    "    # print the title of the article\n",
    "    print(data[i]['title'])\n",
    "    # print the abstract of the article\n",
    "    print(data[i]['abstract'])\n",
    "    # print the body of the article\n",
    "    print(data[i]['body'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pre_processed_data = []\n",
    "\n",
    "for article in data:\n",
    "    # Extract the abstract\n",
    "    # Abstract is a dictionary with two keys: simple_summary and abstract\n",
    "    # Extract both\n",
    "    # Apply validation to check if the key exists\n",
    "    abstract = ''\n",
    "    if 'simple_summary' in article['abstract'] and article['abstract']['simple_summary'] is not None:\n",
    "        abstract += article['abstract']['simple_summary']\n",
    "    if 'abstract' in article['abstract'] and article['abstract']['abstract'] is not None:\n",
    "        abstract += article['abstract']['abstract']\n",
    "\n",
    "    # If the abstract is empty, skip the article\n",
    "    if abstract == '':\n",
    "        continue\n",
    "\n",
    "    # Normalize the abstract\n",
    "    abstract = normalize(abstract)\n",
    "    # Tokenize the abstract\n",
    "    abstract = tokenize(abstract)\n",
    "    # Remove stopwords from the abstract\n",
    "    abstract = remove_stopwords(abstract)\n",
    "    # Apply stemming on the abstract\n",
    "    abstract = stem(abstract)\n",
    "    # Apply lemmatization on the abstract\n",
    "    abstract = lemmatize(' '.join(abstract))\n",
    "    \n",
    "    # Extract the body\n",
    "    # Apply validation to check if the key exists\n",
    "    body = ''\n",
    "    for section in article['body']:\n",
    "        body += ' '.join(section['content'])\n",
    "    # Normalize the body\n",
    "    body = normalize(body)\n",
    "    # Tokenize the body\n",
    "    body = tokenize(body)\n",
    "    # Remove stopwords from the body\n",
    "    body = remove_stopwords(body)\n",
    "    # Apply stemming on the body\n",
    "    body = stem(body)\n",
    "    # Apply lemmatization on the body\n",
    "    body = lemmatize(' '.join(body))\n",
    "\n",
    "    # Create a dictionary to store the data\n",
    "    article_data = {\n",
    "        'abstract': abstract,\n",
    "        'body': body,\n",
    "    }\n",
    "\n",
    "    # Append the dictionary to the list\n",
    "    pre_processed_data.append(article_data)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
